csh
source /usr/local/sge_current/jcvi/common/settings.csh
setenv PATH /usr/local/packages/seq454-64_patch-v2.5p1-internal-10Jun32-1/bin:${PATH}
setenv PATH /usr/local/packages/clc-ngs-cell:/usr/local/packages/clc-bfx-cell:${PATH}
setenv RUBYLIB /usr/local/devel/DAS/users/tstockwe/Ruby/Tools/Bio
use emboss50
umask 002

set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sff_file_list = "\
/local/seq454/2009_04_01/R_2009_03_31_15_30_46_FLX01070134_adminrig_033109R1INFLU36R2INFLUPFLU1/D_2009_04_01_01_06_08_dell-2-0-9_signalProcessing/sff/FTFZ1KV01.sff\
"
set fastq_file_list = "\
"

set sispa_pool_name = 20090205_HIsamples6to100
set sff_file_list = "\
/local/seq454/2009_04_01/R_2009_03_31_16_19_12_FLX02080322_adminrig_033109FULLPLATEINFLUENZA96/D_2009_04_01_13_02_44_dell-2-0-10_signalProcessing/sff/FTF2AAH01.sff,\
/local/seq454/2009_04_01/R_2009_03_31_16_19_12_FLX02080322_adminrig_033109FULLPLATEINFLUENZA96/D_2009_04_01_13_02_44_dell-2-0-10_signalProcessing/sff/FTF2AAH02.sff\
"
set fastq_file_list = "\
/local/solexa_archive1/090922_SOLEXA1_0010/Data/Intensities/Bustard1.4.0_14-10-2009_solexa/GERALD_15-10-2009_solexa/s_1_1_sequence.txt,\
/local/solexa_archive1/090922_SOLEXA1_0010/Data/Intensities/Bustard1.4.0_14-10-2009_solexa/GERALD_15-10-2009_solexa/s_1_2_sequence.txt\
"

set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sff_file_list = "\
/local/seq454/2009_09_18/R_2009_09_18_10_52_19_FLX01070134_adminrig_091809R1M233R2INFLUSISPA/D_2009_09_18_21_06_55_dell-2-0-4_signalProcessing/sff/F2M4HHZ02.sff\
"
set fastq_file_list = "\
"

set sispa_pool_name = 20091005_AVIAN113
set sff_file_list = "\
/local/seq454/2009_12_11/R_2009_12_10_15_12_10_FLX02080319_Administrator_121009R1SISPA113R2GS355LG/D_2009_12_11_04_46_31_dell-2-0-2_signalProcessing/sff/F66E6K401.sff,\
/local/seq454/2009_12_11/R_2009_12_10_15_12_10_FLX02080319_Administrator_121009R1SISPA113R2GS355LG/D_2009_12_11_04_46_31_dell-2-0-2_signalProcessing/sff/F66E6K402.sff\
"
set fastq_file_list = "\
/local/solexa_archive1/091125_SOLEXA1_0003/Data/Intensities/BaseCalls/GERALD_07-12-2009_solexa/s_4_1_sequence.txt,\
/local/solexa_archive1/091125_SOLEXA1_0003/Data/Intensities/BaseCalls/GERALD_07-12-2009_solexa/s_4_2_sequence.txt\
"



set sispa_pool_name = 20091215_MCEIRSsamples1to50
set sff_file_list = "\
/local/seq454/2010_01_09/R_2010_01_08_13_45_41_FLX01070134_adminrig_010810fullplateavianflu/D_2010_01_09_14_20_05_dell-2-0-5_signalProcessing/sff/GAC4IF301.sff,\
/local/seq454/2010_01_09/R_2010_01_08_13_45_41_FLX01070134_adminrig_010810fullplateavianflu/D_2010_01_09_14_20_05_dell-2-0-5_signalProcessing/sff/GAC4IF302.sff\
"
set fastq_file_list = "\
/local/solexa_archive1/100106_SOLEXA1_0007/Data/C1-209_Firecrest1.5.1_19-01-2010_solexa/Bustard1.5.1_19-01-2010_solexa/GERALD_21-01-2010_solexa.2/s_1_1_sequence.txt,\
/local/solexa_archive1/100106_SOLEXA1_0007/Data/C1-209_Firecrest1.5.1_19-01-2010_solexa/Bustard1.5.1_19-01-2010_solexa/GERALD_21-01-2010_solexa.2/s_1_2_sequence.txt,\
/local/solexa_archive1/100106_SOLEXA1_0007/Data/C1-209_Firecrest1.5.1_19-01-2010_solexa/Bustard1.5.1_19-01-2010_solexa/GERALD_21-01-2010_solexa.2/s_2_1_sequence.txt,\
/local/solexa_archive1/100106_SOLEXA1_0007/Data/C1-209_Firecrest1.5.1_19-01-2010_solexa/Bustard1.5.1_19-01-2010_solexa/GERALD_21-01-2010_solexa.2/s_2_2_sequence.txt\
"

set sispa_pool_name = 20100226_80xAK_1xMCE_3xARBO_4xVEEV_4xHOLMES
set sff_file_list = "\
/local/seq454/2010_03_17/R_2010_03_17_08_46_04_FLX02080322_adminrig_031710FULLPLATEAVIANFLU104CEIRS/D_2010_03_17_19_31_11_dell-2-0-2_signalProcessing/sff/GD3XA2R01.sff,\
/local/seq454/2010_03_17/R_2010_03_17_08_46_04_FLX02080322_adminrig_031710FULLPLATEAVIANFLU104CEIRS/D_2010_03_17_19_31_11_dell-2-0-2_signalProcessing/sff/GD3XA2R02.sff\
"
set fastq_file_list = "\
"

set sispa_pool_name = 20100226_B_79xAK_1xMCE_3xARBO_3xVEEV_1xSW
set sff_file_list = "\
/local/seq454/2010_06_10/R_2010_06_10_13_11_11_FLX02080322_adminrig_061010FULLPLATE20100226B/D_2010_06_11_02_34_23_dell-2-0-1_signalProcessing/sff/GIO2WXT01.sff,\
/local/seq454/2010_06_10/R_2010_06_10_13_11_11_FLX02080322_adminrig_061010FULLPLATE20100226B/D_2010_06_11_02_34_23_dell-2-0-1_signalProcessing/sff/GIO2WXT02.sff\
"
set fastq_file_list = "\
/local/solexa_archive1/100420_SOLEXA1_0002/Data/Intensities/BaseCalls/demultiplexed_data_29-04-2010/001/GERALD_29-04-2010_solexa/s_6_sequence.txt,\
/local/solexa_archive1/100420_SOLEXA1_0002/Data/Intensities/BaseCalls/demultiplexed_data_29-04-2010/002/GERALD_29-04-2010_solexa/s_5_sequence.txt\
"
set fastq_file_list = "\
"


set sispa_pool_name = 20100305_1_86xAK_1xSW
set sff_file_list = "\
/local/seq454/2010_03_25/R_2010_03_25_10_35_08_FLX02080319_Administrator_032510R101XPNR2AVIAN3051/D_2010_03_25_23_49_18_dell-2-0-4_signalProcessing/sff/GEIVOUP02.sff\
"
set fastq_file_list = "\
/local/solexa_archive1/100420_SOLEXA1_0002/Data/Intensities/BaseCalls/demultiplexed_data_29-04-2010/001/GERALD_29-04-2010_solexa/s_8_sequence.txt,\
/local/solexa_archive1/100420_SOLEXA1_0002/Data/Intensities/BaseCalls/demultiplexed_data_29-04-2010/003/GERALD_29-04-2010_solexa/s_5_sequence.txt\
"

set sispa_pool_name = 20100305_2_32xAK_24xCOH_1xMCWS_2xKHBAT_1xSW
set sff_file_list = "\
/local/seq454/2010_03_26/R_2010_03_26_11_20_10_FLX02080322_adminrig_032610R101XPOR2AVIAN20003052/D_2010_03_26_21_49_18_dell-2-0-3_signalProcessing/sff/GEKSFWC02.sff\
"
set fastq_file_list = "\
/local/solexa_archive1/100420_SOLEXA1_0002/Data/Intensities/BaseCalls/demultiplexed_data_29-04-2010/001/GERALD_29-04-2010_solexa/s_7_sequence.txt,\
/local/solexa_archive1/100420_SOLEXA1_0002/Data/Intensities/BaseCalls/demultiplexed_data_29-04-2010/001/GERALD_29-04-2010_solexa/s_5_sequence.txt\
"

set sispa_pool_name = 20100416_B_57xMCE_14xAK_5xCOH_4xVEEV_3xINS_1xCC_1xWKS
set sff_file_list = "\
/local/seq454/2010_05_28/R_2010_05_27_16_39_01_FLX01070134_adminrig_052710R1GS399ICER2SISPAPOOL/D_2010_05_28_05_23_41_dell-2-0-2_signalProcessing/sff/GHXKJBR02.sff\
"
set fastq_file_list = "\
/local/solexa_archive1/100708_SOLEXA1_0041/Data/Intensities/BaseCalls/demultiplex_20-07-2010/001/GERALD_20-07-2010_solexa/s_1_1_sequence.txt,\
/local/solexa_archive1/100708_SOLEXA1_0041/Data/Intensities/BaseCalls/demultiplex_20-07-2010/001/GERALD_20-07-2010_solexa/s_1_2_sequence.txt\
"

set sispa_pool_name = 20100528_84xFBS_8xINS_1xJG
set sff_file_list = "\
/usr/local/seq454/2010_08_03/R_2010_08_02_13_14_21_FLX01070134_adminrig_080210R1AVIAN0528R2BZ1175POOL3/D_2010_08_03_02_51_16_dell-2-0-1_signalProcessing/sff/GLKSD7O01.sff,\
/usr/local/seq454/2010_08_06/R_2010_08_05_14_51_08_FLX02080322_adminrig_080510R1AVIAN0528R2AVIAN0628/D_2010_08_06_02_48_00_dell-2-0-4_signalProcessing/sff/GLQGVIA01.sff\
"
set fastq_file_list = "\
/local/solexa_archive1/100708_SOLEXA1_0041/Data/Intensities/BaseCalls/demultiplex_20-07-2010/001/GERALD_20-07-2010_solexa/s_8_1_sequence.txt,\
/local/solexa_archive1/100708_SOLEXA1_0041/Data/Intensities/BaseCalls/demultiplex_20-07-2010/001/GERALD_20-07-2010_solexa/s_8_2_sequence.txt\
"

set sispa_pool_name = 20100628_30xWBC_13xINS_10xNHRCC_9xLS_6xSDIA_5xMCW_3xSGS_3xEB_2xSDIS_2xJMS_2xFBS_2xDW09_2xOHC_1xEB_1xAK
set sff_file_list = "\
/usr/local/seq454/2010_08_06/R_2010_08_05_14_51_08_FLX02080322_adminrig_080510R1AVIAN0528R2AVIAN0628/D_2010_08_06_02_48_00_dell-2-0-4_signalProcessing/sff/GLQGVIA02.sff\
"
set fastq_file_list = "\
/local/solexa_archive1/100708_SOLEXA1_0041/Data/Intensities/BaseCalls/demultiplex_20-07-2010/001/GERALD_20-07-2010_solexa/s_7_1_sequence.txt,\
/local/solexa_archive1/100708_SOLEXA1_0041/Data/Intensities/BaseCalls/demultiplex_20-07-2010/001/GERALD_20-07-2010_solexa/s_7_2_sequence.txt\
"

set sispa_pool_name = 20100707_21xJP06
set sff_file_list = "\
/usr/local/seq454/2010_07_02/R_2010_07_01_14_22_03_FLX01070134_adminrig_070110R1HMP138MID141R2SISPAROT/D_2010_07_02_02_40_14_dell-2-0-3_signalProcessing/sff/GJVRI1302.sff\
"
set fastq_file_list = "\
"

############################ THIS WAS DONE BEFORE!!! - CLEAN UP THE REDO!!!! #############################
set sispa_pool_name = 20090901_2_20xDW09_12xSW_3xCC
set sff_file_list = "\
/local/seq454/2009_09_18/R_2009_09_18_10_52_19_FLX01070134_adminrig_091809R1M233R2INFLUSISPA/D_2009_09_18_21_06_55_dell-2-0-4_signalProcessing/sff/F2M4HHZ02.sff\
"
set fastq_file_list = "\
"
##########################################################################################################

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sispa_data_root = ${project_root}/sispa_data_new
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set sispa_data_dir = ${sispa_data_root}/${sispa_pool_name}
set fastq_dir = ${sispa_data_dir}/fastq
set merged_fastq_dir = ${sispa_data_dir}/merged_fastq
set deconvolved_merged_fastq_dir = ${sispa_data_dir}/deconvolved_merged_fastq
set sff_dir = ${sispa_data_dir}/sff
set merged_sff_dir = ${sispa_data_dir}/merged_sff
set deconvolved_merged_sff_dir = ${sispa_data_dir}/deconvolved_merged_sff
set merged_fastq_file = ${merged_fastq_dir}/merged_solexa_sequence.fastq
set merged_sff_file = ${merged_sff_dir}/merged_454.sff
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt


pushd ${project_root}

if ( -d ${barcode_data_root} ) then
else
  mkdir -p ${barcode_data_root}
endif
if ( -d ${sispa_data_root} ) then
else
  mkdir -p ${sispa_data_root}
endif
if ( -d ${sample_data_root} ) then
else
  mkdir -p ${sample_data_root}
endif

if ( -d ${barcode_data_dir} ) then
else
  mkdir -p ${barcode_data_dir}
endif
if ( -d ${sispa_data_dir} ) then
else
  mkdir -p ${sispa_data_dir}
endif


if ( -d ${fastq_dir} ) then
else
  mkdir -p ${fastq_dir}
endif
if ( -d ${merged_fastq_dir} ) then
else
  mkdir -p ${merged_fastq_dir}
endif
if ( -d ${deconvolved_merged_fastq_dir} ) then
else
  mkdir -p ${deconvolved_merged_fastq_dir}
endif


if ( -d ${sff_dir} ) then
else
  mkdir -p ${sff_dir}
endif
if ( -d ${merged_sff_dir} ) then
else
  mkdir -p ${merged_sff_dir}
endif
if ( -d ${deconvolved_merged_sff_dir} ) then
else
  mkdir -p ${deconvolved_merged_sff_dir}
endif


# build tab-separated-file of barcode metadata based on Excel file attached to 454 BugZero #???
# column order is:
# barcode_name, 
# barcode_sequence, 
# bac_id, 
# blinded_number, 
# species, 
# database_name, 
# collection_name

kedit ${barcode_file_name}

# I used 
# for 20091215_MCEIRSsamples1to50
# cat /usr/local/projects/VHTNGS/sample_data/MCE_1_50_StandardPipeSFF/barcodes/barcode_metadata_from_GLK.txt | \
# gawk '{printf("%s\t%s\t%s\n",$0,"giv3","MCE");}' > ${barcode_file_name}
# for 20090205_HIsamples6to100
# cat /usr/local/projects/VHTNGS/sample_data/HI_6_100_StandardPipeSFF/barcodes/barcode_metadata_from_GLK.txt | \
# gawk '{printf("%s\t%s\t%s\n",$0,"giv3","HI");}' > ${barcode_file_name}
# and for 20091005_AVIAN113
# cat sample_data/AVIAN_113_StandardPipeSFF/barcodes/barcode_metadata_from_GLK.txt | \
#   gawk '{if ($4 ~ /\-AK\-/){c="AK";}\
#          else if($4 ~ /\-RF\-/){c="RF";}\
#          else if($4 ~ /\-SJC\-/){c="SJC";}\
#          else if($4 ~ /\-OHC\-/){c="OHC";}\
#          else if($4 ~ /\-DB\-/){c="DB";}\
#          else if($4 ~ /\_CC\_/){c="CC";}\
#          printf("%s\t%s\t%s\n",$0,"giv3",c);}' > ${barcode_file_name}

if ( -e ${barcode_file_name}.pat ) then
  rm ${barcode_file_name}.pat
endif
touch ${barcode_file_name}.pat
cat ${barcode_file_name} | \
  gawk '{mm=int(length($2)/10.0); printf(">%s <mismatch=%d>\n%s\n", $1, mm, $2);}' \
  >> ${barcode_file_name}.pat




########################## 454 SFF SISPA DATA PROCESSING #####################
# 454 sff data merging, deconvolution, trimming, and non-redundant filtering

# copy the 454 sff data

# merge the 454 sff data using grid resource 

# check that all is finished, and went ok

foreach sff_file (`echo ${sff_file_list} | tr -d ' ' | tr ',' '\n' | sort -u`)
  cp ${sff_file} ${sff_dir}/${sff_file:t}
end

runLinux \
  --commandline "\
    /usr/local/packages/seq454-64_patch-v2.5p1-internal-10Jun32-1/bin/sfffile -o ${merged_sff_file} ${sff_dir}/*.sff\
  " \
  --output ${merged_sff_file}_sfffile_merging.stdout \
  --error ${merged_sff_file}_sfffile_merging.stderr \
  --project 810001 \
  --length fast

cat ${merged_sff_file}_sfffile_merging.std*


############ The NEW WAY - USING GRID DECONVOLVE #########################

set key = `sffinfo ${merged_sff_file} | \
             head -n 100 | \
             grep "Key Sequence:" | \
             cut -d ':' -f 2 | \
             sed -e 's/\s//g' | \
             gawk '{printf("%s\n",$1);}'`
set keylength = `sffinfo ${merged_sff_file} | \
             head -n 100 | \
             grep "Key Sequence:" | \
             cut -d ':' -f 2 | \
             sed -e 's/\s//g' | \
             gawk '{printf("%s\n",length($1));}'`
runLinux \
  --output ${merged_sff_file}_grid_deconvolve.stdout \
  --error ${merged_sff_file}_grid_deconvolve.stderr \
  --project 810001 \
  --length fast \
  --commandline "\
    /usr/local/devel/VIRIFX/software/Grid/bin/grid-deconvolve.pl \
      --project 810001 \
      --infile ${merged_sff_file} \
      --pattern ${barcode_file_name}.pat \
      --queue fast.q \
      --tmpdir ${merged_sff_file}_deconvolver_tmp \
      --outdir ${merged_sff_file}_deconvolver_test \
      --errdir ${merged_sff_file}_deconvolver_err \
      --trim_points_only \
      --readlength 50 \
      --clamplength 6 \
      --keylength 4 \
      --verbose >& ${merged_sff_file}_deconvolver.log \
   "
cat ${merged_sff_file}_deconvolver.log | more

# Now wait until this is finished...  takes a while...  Be sure to check log file(s)


# If any exits due to errors occurred, you will need to re-run the command...

# use the barcode deconvolver output to bin and trim the sff data
foreach bc ( `cat ${barcode_file_name} | cut -f 1`)
  if ( -e ${merged_sff_file}_deconvolver_test/${bc}/${bc}.trim ) then
    echo "INFO: Processing SISPA pool [${sispa_pool_name}] 454 sff data for barcode [${bc}]"
    sfffile -o ${deconvolved_merged_sff_dir}/trim_${bc}.sff \
      -i ${merged_sff_file}_deconvolver_test/${bc}/${bc}.trim \
      -t ${merged_sff_file}_deconvolver_test/${bc}/${bc}.trim \
      ${merged_sff_file}
  endif
end

################################### SOLEXA DATA PROCESSING ####################################

# Solexa fastq data merging, deconvolution, trimming, and non-redundant filtering

# copy the fastq data

# merge the fastq data

# count the number of solexa records


foreach fastq_file (`echo ${fastq_file_list} | tr -d ' ' | tr ',' '\n' | sort -u`)
  cp ${fastq_file} ${fastq_dir}/${fastq_file:t}
end

cat ${fastq_dir}/*_sequence.txt | \
  gawk \
   '{\
      if($0 ~ /^+/) \
      { \
       printf("+\n"); \
      } \
      else \
      { \
       print $0; \
      } \
    }' | \
   /usr/local/projects/VHTNGS/scripts/convert_fastq_illumina_to_fastq_sanger.pl \
  > ${merged_fastq_file}

set sol_rec_cnt = `grep "^@" ${merged_fastq_file} | wc -l`
echo "INFO: Solexa record count is [${sol_rec_cnt}]"

########################### NEW DECONVOLUTION METHOD - using grid_deconvolve.pl ########################### 
# deconvolve and trim the solexa data using the barcode data pattern file

# calculate the number of partitions needed for solexa fastq data

set max_sol_recs_per_deconv = 1000000

if ( ${sol_rec_cnt} > 0 ) then
  @ num_parts = 1 + ${sol_rec_cnt} / ${max_sol_recs_per_deconv}
else
  @ num_parts = 0
endif
echo "INFO: Solexa partition count is [${num_parts}]"

# split solexa fastq file into fastq partition files
@ part = 0
while ( ${part} < ${num_parts} )
  echo "INFO: Building solexa fastq file partition [${part}] of [${num_parts}]"
  cat ${merged_fastq_file} | \
    gawk -v p=${part} -v t=${num_parts} \
      'BEGIN {rec_cnt=0; outflag=0;} \
       {if(((NR % 4) == 1) && ($0 ~ /^@/)){rec_cnt+=1; if((rec_cnt % t) == p ){outflag=1;}else{outflag=0;}} \
        if(outflag==1){print $0;}}' > ${merged_fastq_file}.part_${part}.fastq
  @ part = ${part} + 1
end

# deconvolve and trim the solexa data using the barcode data
@ part = 0
while ( ${part} < ${num_parts} )
  runLinux \
    --output ${merged_fastq_file}_grid_deconvolve.stdout.part_${part} \
    --error ${merged_fastq_file}_grid_deconvolve.stderr.part_${part} \
    --project 810001 \
    --length marathon \
    --nowait \
    --commandline "\
#/usr/local/devel/VIRIFX/software/Grid/bin/grid-deconvolve.pl \ - wait until Nelson fixes main copy
/usr/local/devel/VIRIFX/users/tstockwe/software/Grid/bin/grid-deconvolve.pl \
  --project 810001 \
  --infile ${merged_fastq_file}.part_${part}.fastq \
  --pattern ${barcode_file_name}.pat \
  --queue fast.q \
  --tmpdir ${merged_fastq_file}_deconvolver_tmp.part_${part} \
  --outdir ${merged_fastq_file}_deconvolver_test.part_${part} \
  --errdir ${merged_fastq_file}_deconvolver_err.part_${part} \
  --readlength 50 \
  --clamplength 6 \
  --keylength 0 \
  --verbose >& ${merged_fastq_file}_deconvolver.log.part_${part} \
"
  @ part = ${part} + 1
end

# Try --nocleanup to diagnose the problem
/usr/local/devel/VIRIFX/software/Grid/bin/grid-deconvolve.pl \
  --project 810001 \
  --infile ${merged_fastq_file}.part_${part}.fastq \
  --pattern ${barcode_file_name}.pat \
  --queue fast.q \
  --tmpdir ${merged_fastq_file}_nocleanup_tstockwe_deconvolver_tmp.part_${part} \
  --outdir ${merged_fastq_file}_nocleanup_tstockwe_deconvolver_test.part_${part} \
  --errdir ${merged_fastq_file}_nocleanup_tstockwe_deconvolver_err.part_${part} \
  --readlength 50 \
  --clamplength 6 \
  --keylength 0 \
  --nocleanup \
  --verbose >& ${merged_fastq_file}_nocleanup_tstockwe_deconvolver.log.part_${part}

# I made a copy and made mods for /tmp to /usr/local/scratch, HOSTNAME output (debug), and fully specified fuzznuc
/usr/local/devel/VIRIFX/users/tstockwe/software/Grid/bin/grid-deconvolve.pl \
  --project 810001 \
  --infile ${merged_fastq_file}.part_${part}.fastq \
  --pattern ${barcode_file_name}.pat \
  --queue fast.q \
  --tmpdir ${merged_fastq_file}_nocleanup_tstockwe_deconvolver_tmp.part_${part} \
  --outdir ${merged_fastq_file}_nocleanup_tstockwe_deconvolver_test.part_${part} \
  --errdir ${merged_fastq_file}_nocleanup_tstockwe_deconvolver_err.part_${part} \
  --readlength 50 \
  --clamplength 6 \
  --keylength 0 \
  --nocleanup \
  --verbose >& ${merged_fastq_file}_nocleanup_tstockwe_deconvolver.log.part_${part}

# Now wait until all the parts are finished
ls -1 ${merged_fastq_file}_deconvolver.log.part_*
tail -n 1 ${merged_fastq_file}_deconvolver.log.part_*
more ${merged_fastq_file}_deconvolver.log.part_*

@ part = 0
while ( ${part} < ${num_parts} )
  foreach bc ( `cat ${barcode_file_name} | cut -f 1`)
    echo "INFO:  moving data for barcode [${bc}] part [${part}] of [${num_parts}]"
    set in_fastq = ${merged_fastq_file}_deconvolver_test.part_${part}/${bc}/${bc}.fastq
    set out_fastq = ${merged_fastq_file}_deconvolver_test.part_${part}/${bc}.fastq
    set in_trimpoints = ${merged_fastq_file}_deconvolver_test.part_${part}/${bc}/${bc}.trim
    set out_trimpoints = ${merged_fastq_file}_deconvolver_test.part_${part}/trim_${bc}.txt
    if ( -e ${in_fastq} ) then
      echo "INFO:  mv ${in_fastq} ${out_fastq}"
      mv ${in_fastq} ${out_fastq}
    else
      echo "ERROR:  [${in_fastq}] does not exist"
    endif
    if ( -e ${in_trimpoints} ) then
      echo "INFO:  mv ${in_trimpoints} ${out_trimpoints}"
      mv ${in_trimpoints} ${out_trimpoints}
    else
      echo "ERROR:  [${in_trimpoints}] does not exist"
    endif
  end
  @ part = ${part} + 1
end


# combine the individual partitions of deconvolved and trimmed solexa data by barcode
# and then make the data non-redundant
foreach bc ( `cat ${barcode_file_name} | cut -f 1`)
  set deconvolved_fastq = ${deconvolved_merged_fastq_dir}/trim_${bc}.fastq
  set deconvolved_fastq_trimpoints = ${deconvolved_merged_fastq_dir}/trim_${bc}.fastq.trimpoints
  set deconvolved_fastq_untrimmed = ${deconvolved_merged_fastq_dir}/trim_${bc}.fastq.untrimmed
  set nr_deconvolved_fastq = ${deconvolved_merged_fastq_dir}/nr_trim_${bc}.fastq

  if ( -e ${deconvolved_fastq} ) then
    rm ${deconvolved_fastq}
  endif
  touch ${deconvolved_fastq}

  if ( -e ${deconvolved_fastq_trimpoints} ) then
    rm ${deconvolved_fastq_trimpoints}
  endif
  touch ${deconvolved_fastq_trimpoints}

  if ( -e ${deconvolved_fastq_untrimmed} ) then
    rm ${deconvolved_fastq_untrimmed}
  endif
  touch ${deconvolved_fastq_untrimmed}

  @ part = 0
  while ( ${part} < ${num_parts} )
    echo "INFO: Accumulating Illumina/Solexa trimmed fastq and trimpoint files, partition [${part}] of [${num_parts}] for barcode [${bc}]"
    set file_cnt = `ls -1 ${merged_fastq_file}_deconvolver_test.part_${part}/${bc}*.fastq | wc -l`
    if ( ${file_cnt} > 0  ) then
      cat ${merged_fastq_file}_deconvolver_test.part_${part}/${bc}*.fastq >> ${deconvolved_fastq}
    else
      echo "WARNING:  [${merged_fastq_file}_deconvolver_test.part_${part}/${bc}*.fastq] does not exist"
    endif

    set file_cnt = `ls -1 ${merged_fastq_file}_deconvolver_test.part_${part}/trim_${bc}*.txt | wc -l`
    if ( ${file_cnt} > 0  ) then
      cat ${merged_fastq_file}_deconvolver_test.part_${part}/trim_${bc}*.txt | \
        gawk -F'\t' '{if(NF>2){printf("%s\t%s\t%s\n",$(NF-2),$(NF-1),$(NF));}}' >> ${deconvolved_fastq_trimpoints}
    else
      echo "WARNING:  [${merged_fastq_file}_deconvolver_test.part_${part}/trim_${bc}*.txt] does not exist"
    endif

    @ part = ${part} + 1
  end

  sort ${deconvolved_fastq_trimpoints} | tr '_' ':' | sed -e 's/:/_/' > ${deconvolved_fastq_trimpoints}.sorted
  mv ${deconvolved_fastq_trimpoints}.sorted ${deconvolved_fastq_trimpoints}

  if ( `cat ${deconvolved_fastq_trimpoints} | wc -l` > 0 ) then
    /usr/local/devel/DAS/software/JavaCommon2/fastQfile.pl \
      -o ${deconvolved_fastq_untrimmed} \
      -i ${deconvolved_fastq_trimpoints} \
      ${merged_fastq_file}
  endif

  cat ${deconvolved_fastq_untrimmed} | \
    gawk '{t=NR % 4;\
           if(t==1){\
             if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,sid,q)};\
             sid=substr($0,2);\
             gsub("_",":",sid);\
             sub(":","_",sid);\
           }\
           else if (t==2){s=$0;}\
           else if (t==3){qid=sid;}\
           else if (t==0){q=$0;}\
          }\
          END {\
            if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,sid,q)};\
          }' | \
    sort | \
    gawk -F'\t' '{printf("@%s\n%s\n+%s\n%s\n", $1, $2, "", $4);}' > ${deconvolved_fastq_untrimmed}.sorted
  mv ${deconvolved_fastq_untrimmed} ${deconvolved_fastq_untrimmed}.unsorted
  mv ${deconvolved_fastq_untrimmed}.sorted ${deconvolved_fastq_untrimmed}

  cat ${deconvolved_fastq} | \
    gawk '{t=NR % 4;\
           if(t==1){\
             if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", s,sid,qid,q)};\
             sid=$0;\
           }\
           else if (t==2){s=$0;}\
           else if (t==3){qid=$0;}\
           else if (t==0){q=$0;}\
          }\
          END {\
            if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", s,sid,qid,q)};\
          }' | \
    sort | \
    gawk -F'\t' '{if($1!=last && index($1,"N")==0){print $0;last=$1}}'  | \
    sort --key=2,2 -i | \
    gawk -F'\t' '{printf("%s\t%s\t%s\t%s\n", $2, $1, $3, $4);}' | \
    sort --key=1,1 -i | \
    gawk -F'\t' '{printf("%s\n%s\n%s\n%s\n", $1, $2, "", $4);}' > ${nr_deconvolved_fastq}

  cat ${nr_deconvolved_fastq} | \
    gawk '{t=NR % 4;\
           if(t==1){\
             if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,sid,q)};\
             sid=substr($0,2);\
             gsub("_",":",sid);\
             sub(":","_",sid);\
           }\
           else if (t==2){s=$0;}\
           else if (t==3){qid=sid;}\
           else if (t==0){q=$0;}\
          }\
          END {\
             if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,sid,q)};\
          }' | \
    sort | \
    gawk -F'\t' '{printf("@%s\n%s\n+%s\n%s\n", $1, $2, "", $4);}' \
      > ${nr_deconvolved_fastq}.sorted
  mv ${nr_deconvolved_fastq} ${nr_deconvolved_fastq}.unsorted
  mv ${nr_deconvolved_fastq}.sorted ${nr_deconvolved_fastq}

end

############################# END SOLEXA DATA PROCESSING ###############################

############################# COPY SISPA DATA TO SAMPLE AREAS ##########################
foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':' ` )
  set bc       = `echo "${bc_rec}" | cut -d ':' -f 1`
  set bc_seq   = `echo "${bc_rec}" | cut -d ':' -f 2`
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set blinded  = `echo "${bc_rec}" | cut -d ':' -f 4`
  set species  = `echo "${bc_rec}" | cut -d ':' -f 5`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 6`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 7`
  set bac_id_len = `echo ${bac_id} | tr -d '\n' | wc -c`
  set db_name_len = `echo ${db_name} | tr -d '\n' | wc -c`
  set col_name_len = `echo ${col_name} | tr -d '\n' | wc -c`
  if (${bac_id_len} > 0 && ${db_name_len} > 0 && ${col_name_len} > 0) then
    echo "INFO: processing data for SISPA pool [${sispa_pool_name}] barcode [${bc}]"
    set deconvolved_fastq_trimpoints = ${deconvolved_merged_fastq_dir}/trim_${bc}.fastq.trimpoints
    set deconvolved_fastq_untrimmed = ${deconvolved_merged_fastq_dir}/trim_${bc}.fastq.untrimmed
    set nr_deconvolved_fastq = ${deconvolved_merged_fastq_dir}/nr_trim_${bc}.fastq
    set deconvolved_sff = ${deconvolved_merged_sff_dir}/trim_${bc}.sff
    set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}

    set sample_data_solexa = ${sample_data}/solexa
    set sample_data_sff = ${sample_data}/sff

    if ( -d ${sample_data} ) then
    else
      mkdir -p ${sample_data}
    endif

    if ( -d ${sample_data_solexa} ) then
    else
      mkdir -p ${sample_data_solexa}
    endif

    if ( -d ${sample_data_sff} ) then
    else
      mkdir -p ${sample_data_sff}
    endif

    if ( -e ${nr_deconvolved_fastq} ) then
      echo "INFO: copying fastq data to [${db_name}/${col_name}/${bac_id}]"
      cp ${deconvolved_fastq_trimpoints} ${sample_data_solexa}/${sispa_pool_name}_trim_${bc}.fastq.trimpoints
      cp ${deconvolved_fastq_untrimmed} ${sample_data_solexa}/${sispa_pool_name}_trim_${bc}.fastq.untrimmed
      cp ${nr_deconvolved_fastq} ${sample_data_solexa}/${sispa_pool_name}_nr_trim_${bc}.fastq
    endif

    if ( -e ${deconvolved_sff} ) then
      echo "INFO: copying sff data to [${db_name}/${col_name}/${bac_id}]"
      set key = `sffinfo ${deconvolved_sff} | \
                   head -n 100 | \
                   grep "Key Sequence:" | \
                   cut -d ':' -f 2 | \
                   sed -e 's/\s//g' | \
                   gawk '{printf("%s\n",$1);}'`
      cp ${deconvolved_sff} ${sample_data_sff}/${sispa_pool_name}_trim_${bc}.${key}.sff
    endif
  else
    echo "WARNING:  No sample data transfer for bc_rec [${bc_rec}]"
  endif
end

########################## CONSOLIDATE SAMPLE DATA ##################################
csh
set sispa_pool_name = 20090205_HIsamples6to100
set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sispa_pool_name = 20091005_AVIAN113
set sispa_pool_name = 20091215_MCEIRSsamples1to50
set sispa_pool_name = 20100305_1_86xAK_1xSW
set sispa_pool_name = 20100305_2_32xAK_24xCOH_1xMCWS_2xKHBAT_1xSW
set sispa_pool_name = 20100416_B_57xMCE_14xAK_5xCOH_4xVEEV_3xINS_1xCC_1xWKS
set sispa_pool_name = 20100226_B_79xAK_1xMCE_3xARBO_3xVEEV_1xSW
set sispa_pool_name = 20100528_84xFBS_8xINS_1xJG

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt

# set barcode_file_name = ${project_root}/ALL_SISPA_POOLS_${bc_part}_of_${num_bc_parts}_barcode_metadata_from_GLK.txt
# set barcode_file_name = ${project_root}/HAS_SANGER_barcode_metadata_from_GLK.txt

foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':' `)
  set bc       = `echo "${bc_rec}" | cut -d ':' -f 1`
  set bc_seq   = `echo "${bc_rec}" | cut -d ':' -f 2`
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set blinded  = `echo "${bc_rec}" | cut -d ':' -f 4`
  set species  = `echo "${bc_rec}" | cut -d ':' -f 5`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 6`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 7`
  echo "INFO: processing data for [${db_name}/${col_name}/${bac_id}]"

  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}
  set sample_data_solexa = ${sample_data}/solexa
  set sample_data_sff = ${sample_data}/sff
  set sample_data_sanger = ${sample_data}/sanger
  set sample_mapping_dir = ${sample_data}/mapping
  set final_fasta_reads = ${sample_mapping_dir}/${db_name}_${col_name}_${bac_id}_final.fasta
  set sample_data_merged_solexa = ${sample_data}/merged_solexa
  set sample_data_merged_sff = ${sample_data}/merged_sff
  set sample_data_merged_sanger = ${sample_data}/merged_sanger

  set sample_data_merged_sff_file = ${sample_data_merged_sff}/${db_name}_${col_name}_${bac_id}.sff
  set sample_data_merged_sanger_file = ${sample_data_merged_sanger}/${db_name}_${col_name}_${bac_id}.fasta
  set sample_data_merged_solexa_file = ${sample_data_merged_solexa}/${db_name}_${col_name}_${bac_id}.fastq
  set sample_data_merged_solexa_file_t = ${sample_data_merged_solexa}/${db_name}_${col_name}_${bac_id}.fastq.trimpoints
  set sample_data_merged_solexa_file_u = ${sample_data_merged_solexa}/${db_name}_${col_name}_${bac_id}.fastq.untrimmed

  if ( -d ${sample_data_sanger} ) then
  else
    mkdir -p ${sample_data_sanger}
  endif

  if ( -e ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta ) then
    if ( `cat ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta | wc -l` > 0 ) then
    else
      echo "WARNING: No Sanger fasta file exists for [${db_name}/${col_name}/${bac_id}]"
      touch ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta
      touch ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta.untrimmed
      touch ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta.trimpoints
    endif
  else
    echo "WARNING: No Sanger fasta file exists for [${db_name}/${col_name}/${bac_id}]"
    touch ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta
    touch ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta.untrimmed
    touch ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta.trimpoints
  endif

  if ( -d ${sample_data_merged_solexa} ) then
  else
    mkdir -p ${sample_data_merged_solexa}
  endif

  if ( -d ${sample_data_merged_sff} ) then
  else
    mkdir -p ${sample_data_merged_sff}
  endif

  if ( -d ${sample_data_merged_sanger} ) then
  else
    mkdir -p ${sample_data_merged_sanger}
  endif

  cat ${sample_data_solexa}/*_nr_trim_*.fastq | \
    gawk '{t=NR % 4;\
           if(t==1){\
             if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,sid,q)};\
             sid=substr($0,2);\
             gsub("_",":",sid);\
             sub(":","_",sid);\
           }\
           else if (t==2){s=$0;}\
           else if (t==3){qid=sid;}\
           else if (t==0){q=$0;}\
          }\
          END {\
             if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,sid,q)};\
          }' | \
    sort | \
    gawk -F'\t' '{printf("@%s\n%s\n+%s\n%s\n", $1, $2, "", $4);}' \
      > ${sample_data_merged_solexa_file}

  cat ${sample_data_solexa}/*_trim_*.fastq.trimpoints | \
    sort \
    > ${sample_data_merged_solexa_file_t}

  cat ${sample_data_solexa}/*_trim_*.fastq.untrimmed | \
    gawk '{t=NR % 4;\
           if(t==1){\
             if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,sid,q)};\
             sid=substr($0,2);\
             gsub("_",":",sid);\
             sub(":","_",sid);\
           }\
           else if (t==2){s=$0;}\
           else if (t==3){qid=sid;}\
           else if (t==0){q=$0;}\
          }\
          END {\
             if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,sid,q)};\
          }' | \
    sort | \
    gawk -F'\t' '{printf("@%s\n%s\n+%s\n%s\n", $1, $2, "", $4);}' \
      > ${sample_data_merged_solexa_file_u}

  foreach key (`ls -1 ${sample_data_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
    sfffile -o ${sample_data_merged_sff_file:r}.${key}.sff \
      ${sample_data_sff}/*_trim_*.${key}.sff
  end

  cp ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta ${sample_data_merged_sanger_file}
  cp ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta.untrimmed ${sample_data_merged_sanger_file}.untrimmed
  cp ${sample_data_sanger}/${db_name}_${col_name}_${bac_id}_final.fasta.trimpoints ${sample_data_merged_sanger_file}.trimpoints
end

################### THIS IS THE START OF VIRUS SPECIFIC HANDLING ###############
csh
setenv PATH /usr/local/packages/clc-ngs-cell:/usr/local/packages/clc-bfx-cell:${PATH}
setenv RUBYLIB /usr/local/devel/DAS/users/tstockwe/Ruby/Tools/Bio
use emboss50
umask 002

set sispa_pool_name = 20090205_HIsamples6to100
set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sispa_pool_name = 20091005_AVIAN113
set sispa_pool_name = 20091215_MCEIRSsamples1to50
set sispa_pool_name = 20100305_1_86xAK_1xSW
set sispa_pool_name = 20100305_2_32xAK_24xCOH_1xMCWS_2xKHBAT_1xSW
set sispa_pool_name = 20100416_B_57xMCE_14xAK_5xCOH_4xVEEV_3xINS_1xCC_1xWKS
set sispa_pool_name = 20100226_B_79xAK_1xMCE_3xARBO_3xVEEV_1xSW
set sispa_pool_name = 20100528_84xFBS_8xINS_1xJG

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt


# under ref_dir, look for <seg>.fasta
# under blast_db_dir, look for <seg>_full_length_NT_complete.fa, must have been run through formatdb!
# e.g., formatdb -i MAIN_full_length_NT_complete.fa -p F -o T 
# cat veev_full_length_NT/MAIN_full_length_NT_complete.fa | \
#   gawk -v l=0 '{if($0 ~ />/){l=l+1;printf(">MAIN_%d %s\n",l,substr($0,2));}else{print $0;}}' > \
#   veev/MAIN.fasta
# 
set segments = "VP1 VP2 VP3 VP4 NSP1 VP6 NSP3 NSP2 VP7 NSP4 NSP5"
foreach seg ( `echo ${segments} | tr ' ' '\n'` )
  grep ${seg} /home/tstockwe/for_rtv/Tim_rotavirus_seq_segAssignment.txt | cut -d '.' -f 1,2 > ${seg}_accessions.list
  fnafile -i /home/tstockwe/for_rtv/${seg}_accessions.list -o /home/tstockwe/for_rtv/${seg}.fasta /home/tstockwe/for_rtv/ALLRECORDS.fna
  cat /home/tstockwe/for_rtv/${seg}.fasta | \
    gawk -v s=${seg} -v l=0 \
      '{if($0 ~ />/){l=l+1;printf(">%s_%d %s\n",s, l,substr($0,2));}else{print $0;}}' > \
    /usr/local/projects/VHTNGS/reference_data/rota_virus/${seg}.fasta
  cat /home/tstockwe/for_rtv/${seg}.fasta > \
    /usr/local/projects/VHTNGS/reference_data/rota_virus_full_length_NT/${seg}_full_length_NT_complete.fa
  pushd /usr/local/projects/VHTNGS/reference_data/rota_virus_full_length_NT  >& /dev/null
    formatdb -i ${seg}_full_length_NT_complete.fa -p F -o T 
  popd >& /dev/null
end

set segments = "VP1 VP2 VP3 VP4 NSP1 VP6 NSP3 NSP2 VP7 NSP4 NSP5"
foreach seg ( `echo ${segments} | tr ' ' '\n'` )
  cat /home/tstockwe/for_rtv/uclust_work/${seg}_AllCompleteCDSs_fasta_sorted_clusters_sorted_uc_accessions.fasta | \
    gawk -v s=${seg} -v l=0 \
      '{if($0 ~ />/){l=l+1;printf(">%s_%d %s\n",s, l,substr($0,2));}else{print $0;}}' > \
    /usr/local/projects/VHTNGS/reference_data/rota_virus/${seg}.fasta
  cat /home/tstockwe/for_rtv/uclust_work/${seg}_AllCompleteCDSs_fasta_sorted_clusters_sorted_uc_accessions.fasta > \
    /usr/local/projects/VHTNGS/reference_data/rota_virus_full_length_NT/${seg}_full_length_NT_complete.fa
  pushd /usr/local/projects/VHTNGS/reference_data/rota_virus_full_length_NT  >& /dev/null
    formatdb -i ${seg}_full_length_NT_complete.fa -p F -o T 
  popd >& /dev/null
end

# seg_cov is the segment followed by bps in 100x coverage
# for unsegmented viruses, use MAIN as segment name

# set barcode_file_name = ${project_root}/ALL_SISPA_POOLS_${bc_part}_of_${num_bc_parts}_barcode_metadata_from_GLK.txt
# set barcode_file_name = ${project_root}/ALL_SISPA_POOLS_barcode_metadata_from_GLK.txt
# set barcode_file_name = ${project_root}/HAS_SANGER_${bc_part}_of_${num_bc_parts}_barcode_metadata_from_GLK.txt

foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':' ` )
  set bc       = `echo "${bc_rec}" | cut -d ':' -f 1`
  set bc_seq   = `echo "${bc_rec}" | cut -d ':' -f 2`
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set blinded  = `echo "${bc_rec}" | cut -d ':' -f 4`
  set species  = `echo "${bc_rec}" | cut -d ':' -f 5`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 6`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 7`

  switch ($db_name)
    case giv:
      echo "Using Influenza A reference data for database [${db_name}]"
      set ref_dir = /usr/local/projects/VHTNGS/reference_data/influenza_a_virus
      set blast_db_dir = /usr/local/projects/GIV/Influenza_full_length_NT
      set segments = "HA MP NA NP NS PA PB1 PB2"
      set seg_cov = "HA:175000 MP:100000 NA:145000 NP:155000 NS:89000 PA:220000 PB1:235000 PB2:235000"
      set flu_a = 1
    breaksw
    case giv3:
      echo "Using Influenza A reference data for database [${db_name}]"
      set ref_dir = /usr/local/projects/VHTNGS/reference_data/influenza_a_virus
      set blast_db_dir = /usr/local/projects/GIV/Influenza_full_length_NT
      set segments = "HA MP NA NP NS PA PB1 PB2"
      set seg_cov = "HA:175000 MP:100000 NA:145000 NP:155000 NS:89000 PA:220000 PB1:235000 PB2:235000"
      set flu_a = 1
    breaksw
    case piv:
      echo "Using Influenza A reference data for database [${db_name}]"
      set ref_dir = /usr/local/projects/VHTNGS/reference_data/influenza_a_virus
      set blast_db_dir = /usr/local/projects/GIV/Influenza_full_length_NT
      set segments = "HA MP NA NP NS PA PB1 PB2"
      set seg_cov = "HA:175000 MP:100000 NA:145000 NP:155000 NS:89000 PA:220000 PB1:235000 PB2:235000"
      set flu_a = 1
    breaksw
    case swiv:
      echo "Using Influenza A reference data for database [${db_name}]"
      set ref_dir = /usr/local/projects/VHTNGS/reference_data/influenza_a_virus
      set blast_db_dir = /usr/local/projects/GIV/Influenza_full_length_NT
      set segments = "HA MP NA NP NS PA PB1 PB2"
      set seg_cov = "HA:175000 MP:100000 NA:145000 NP:155000 NS:89000 PA:220000 PB1:235000 PB2:235000"
      set flu_a = 1
    breaksw
    case rtv:
      echo "Using Rotavirus reference data for database [${db_name}]"
      set ref_dir      = /usr/local/projects/VHTNGS/reference_data/rota_virus
      set blast_db_dir = /usr/local/projects/VHTNGS/reference_data/rota_virus_full_length_NT
      set segments = "VP1 VP2 VP3 VP4 NSP1 VP6 NSP3 NSP2 VP7 NSP4 NSP5"
      set seg_cov = "VP1:326700 VP2:268600 VP3:255000 VP4:232400 NSP1:151800 VP6:132300 NSP3:104100 NSP2:102200 VP7:103000 NSP4:70800 NSP5:62900"
      set flu_a = 0
    breaksw
    case gcv:
      echo "Using Coronavirus reference data for database [${db_name}]"
      set ref_dir      = /usr/local/projects/VHTNGS/reference_data/corona_virus
      set blast_db_dir = /usr/local/projects/VHTNGS/reference_data/corona_virus_full_length_NT
      set segments = "MAIN"
      set seg_cov = "MAIN:3000000"
      set flu_a = 0
    breaksw
    case veev:
      echo "Using VEEV reference data for database [${db_name}]"
      set ref_dir      = /usr/local/projects/VHTNGS/reference_data/veev
      set blast_db_dir = /usr/local/projects/VHTNGS/reference_data/veev_full_length_NT
      set segments = "MAIN"
      set seg_cov = "MAIN:1200000"
      set flu_a = 0
    breaksw
  endsw

  echo "INFO: processing data for [${db_name}/${col_name}/${bac_id}]"
  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}


  set sample_data_merged_solexa = ${sample_data}/merged_solexa
  set sample_data_merged_sff = ${sample_data}/merged_sff
  set sample_data_merged_sanger = ${sample_data}/merged_sanger
  set sample_data_merged_solexa_file = ${sample_data_merged_solexa}/${db_name}_${col_name}_${bac_id}.fastq
  set sample_data_merged_solexa_file_t = ${sample_data_merged_solexa}/${db_name}_${col_name}_${bac_id}.fastq.trimpoints
  set sample_data_merged_solexa_file_u = ${sample_data_merged_solexa}/${db_name}_${col_name}_${bac_id}.fastq.untrimmed
  set sample_data_merged_sff_file = ${sample_data_merged_sff}/${db_name}_${col_name}_${bac_id}.sff
  set sample_data_merged_sanger_file = ${sample_data_merged_sanger}/${db_name}_${col_name}_${bac_id}.fasta

  if ( -e ${sample_data_merged_solexa_file}.fasta ) then
    rm ${sample_data_merged_solexa_file}.fasta
  endif
  touch ${sample_data_merged_solexa_file}.fasta

  if ( -e ${sample_data_merged_solexa_file}.fasta.qual ) then
    rm ${sample_data_merged_solexa_file}.fasta.qual
  endif
  touch ${sample_data_merged_solexa_file}.fasta.qual

  if ( `cat ${sample_data_merged_solexa_file} | wc -l` > 0 ) then
    echo "INFO: converting fastq to fasta for [${db_name}/${col_name}/${bac_id}]"
    if ( -e convert.fa ) then
      rm convert.fa
    endif
    if ( -e convert.qual ) then
      rm convert.qual
    endif
    /usr/local/devel/DAS/users/tstockwe/Ruby/Tools/Bio/fastq2seqQualFasta.rb \
      convert \
      ${sample_data_merged_solexa_file}
    cp convert.fa ${sample_data_merged_solexa_file}.fasta
    cp convert.qual ${sample_data_merged_solexa_file}.fasta.qual
    rm convert.fa
    rm convert.qual
  endif

  echo "INFO: converting sff to fasta for [${db_name}/${col_name}/${bac_id}]"
  if ( -e ${sample_data_merged_sff_file}.fna ) then
    rm ${sample_data_merged_sff_file}.fna
  endif
  touch ${sample_data_merged_sff_file}.fna
  foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
    sffinfo -s ${sample_data_merged_sff_file:r}.${key}.sff | \
      grep -v " length=0 " \
      >> ${sample_data_merged_sff_file}.fna 
  end

  if ( `cat ${sample_data_merged_sff_file}.fna | wc -l` > 0 ) then
    echo "INFO: formatdb of SFF fasta for [${db_name}/${col_name}/${bac_id}]"
    formatdb -p F -i ${sample_data_merged_sff_file}.fna
  endif

  if ( `cat ${sample_data_merged_sanger_file} | wc -l` > 0 ) then
    echo "INFO: formatdb of Sanger fasta for [${db_name}/${col_name}/${bac_id}]"
    formatdb -p F -i ${sample_data_merged_sanger_file}
  endif

  set tblastx_outdir = ${sample_data}/tblastx_output
  if ( -d ${tblastx_outdir} ) then
  else
    mkdir -p ${tblastx_outdir}
  endif

  foreach seg ( `echo ${segments} | tr ' ' '\n' ` )
    echo "INFO: tblastx segment data [${seg}] against SFF and Sanger reads databases for [${db_name}/${col_name}/${bac_id}]"
    set ref_fna = ${ref_dir}/${seg}.fasta

    if ( `cat ${sample_data_merged_sff_file}.fna | wc -l` > 0 ) then
      set blastdb = ${sample_data_merged_sff_file}.fna
      set blastout = ${tblastx_outdir}/${seg}.out
      blastall \
        -p tblastx \
        -d ${blastdb} \
        -i ${ref_fna} \
        -m 9 \
        -b 100000 \
        -v 100000 \
        -o ${blastout}
    else
      touch ${blastout}
    endif

    if ( `cat ${sample_data_merged_sanger_file} | wc -l` > 0 ) then
      set blastdb = ${sample_data_merged_sanger_file}
      set blastout = ${tblastx_outdir}/${seg}_sanger.out
      blastall \
        -p tblastx \
        -d ${blastdb} \
        -i ${ref_fna} \
        -m 9 \
        -b 100000 \
        -v 100000 \
        -o ${blastout}
    else
      touch ${blastout}
    endif
  end

  set noninter_chimera_list = ${tblastx_outdir}/noninter_chimera_reads.uaccno_list
  set inter_chimera_list = ${tblastx_outdir}/inter_chimera_reads.uaccno_list
  foreach seg ( `echo ${segments} | tr ' ' '\n' ` )
    echo "INFO: parsing tblastx output for segment [${seg}] against [${db_name}/${col_name}/${bac_id}]"
    set blastout = ${tblastx_outdir}/${seg}.out
    set blastout_sanger = ${tblastx_outdir}/${seg}_sanger.out
    set nonintra_chimera_list = ${tblastx_outdir}/${seg}_nonintra_chimera_reads.uaccno_list
    set intra_chimera_list = ${tblastx_outdir}/${seg}_intra_chimera_reads.uaccno_list
    if ( ${flu_a} > 0 ) then
      cat ${blastout} ${blastout_sanger} | \
        gawk '{if($0 !~ "#" && $3>60 && $4 > 25 ) {if(($7-$8)*($9-$10)>0){o=1;d=$8-$10;}else{o=0;d=$8+$10;}{printf("%s\t%s\t%s\t%06d\n",$1,$2, o,d);}}}' | \
        sort -g | \
        uniq | \
        gawk '{if( $1==q && $2==s && $3==o && sqrt(($4-d)^2) < 6){d=$4;}else {print $0;q=$1;s=$2;o=$3;d=$4;}}' | \
        cut -f 1,2 | \
        uniq -c | \
        grep " 1 " | \
        gawk '{print $3}' | \
        sort | \
        uniq > ${nonintra_chimera_list}
      cat ${blastout} ${blastout_sanger} | \
        gawk '{if($0 !~ "#" && $3>60 && $4 > 25 ) {if(($7-$8)*($9-$10)>0){o=1;d=$8-$10;}else{o=0;d=$8+$10;}{printf("%s\t%s\t%s\t%06d\n",$1,$2, o,d);}}}' | \
        sort -g | \
        uniq | \
        gawk '{if( $1==q && $2==s && $3==o && sqrt(($4-d)^2) < 6){d=$4;}else {print $0;q=$1;s=$2;o=$3;d=$4;}}' | \
        cut -f 1,2 | \
        uniq -c | \
        grep -v " 1 " | \
        gawk '{print $3}' | \
        sort | \
        uniq > ${intra_chimera_list}
    else
      cat ${blastout} ${blastout_sanger} | \
        gawk '{if($0 !~ "#" && $3>60 && $4 > 25 ) {print $2;}}' | \
        sort | \
        uniq > ${nonintra_chimera_list}
      echo "" > ${intra_chimera_list}     
    endif
  end
  cat ${tblastx_outdir}/*_nonintra_chimera_reads.uaccno_list | \
    sort | \
    uniq -c | \
    tr '\t' ' ' | \
    grep " 1 " | \
    gawk '{print $2}' > ${noninter_chimera_list}
  cat ${tblastx_outdir}/*_nonintra_chimera_reads.uaccno_list | \
    sort | \
    uniq -c | \
    tr '\t' ' ' | \
    grep -v " 1 " | \
    gawk '{print $2}' > ${inter_chimera_list}

  foreach seg ( `echo ${segments} | tr ' ' '\n' ` )
    set nonintra_chimera_list = ${tblastx_outdir}/${seg}_nonintra_chimera_reads.uaccno_list
    set non_chimera_list = ${tblastx_outdir}/${seg}_nonchimera_reads.uaccno_list
    join -1 1 -2 1 \
      ${noninter_chimera_list} \
      ${nonintra_chimera_list} > \
      ${non_chimera_list}
    echo "INFO: creating sff of non_chimeric reads from reads matching segment [${seg}] for [${db_name}/${col_name}/${bac_id}]"

    set sample_seg_sff_file = ${sample_data_merged_sff}/${db_name}_${col_name}_${bac_id}_nonchimera_${seg}.sff
    foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
      sfffile \
        -i ${non_chimera_list} \
        -o ${sample_seg_sff_file:r}.${key}.sff \
        ${sample_data_merged_sff_file:r}.${key}.sff
    end

    set sample_seg_sanger_file = ${sample_data_merged_sanger}/${db_name}_${col_name}_${bac_id}_nonchimera_${seg}.fasta
    if ( `cat ${sample_data_merged_sanger_file} | wc -l` > 0 ) then
      echo "INFO: creating Sanger fasta of non_chimeric reads from reads matching segment [${seg}] for [${db_name}/${col_name}/${bac_id}]"
      fnafile \
        -i ${non_chimera_list} \
        -o ${sample_seg_sanger_file} \
        ${sample_data_merged_sanger_file}
    endif

    echo "INFO: creating 100x max coverage sff of non_chimeric reads from reads matching segment [${seg}] for [${db_name}/${col_name}/${bac_id}]"
    set bps = `echo ${seg_cov} | tr ' ' '\n' | grep ${seg} | cut -d ':' -f 2`
    set sample_seg_100x_sff_file = ${sample_data_merged_sff}/${db_name}_${col_name}_${bac_id}_nonchimera_${seg}_100x.sff
    foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
      sfffile \
        -pick ${bps} \
        -o ${sample_seg_100x_sff_file:r}.${key}.sff \
        ${sample_seg_sff_file:r}.${key}.sff
    end

    set seg_assembly_dir = ${sample_data}/assembly_by_segment/${seg}
    if ( -d ${seg_assembly_dir} ) then
    else
      mkdir -p ${seg_assembly_dir}
    endif

    pushd ${seg_assembly_dir} >& /dev/null
      echo "INFO: performing de novo assembly of 100x coverage for nonchimera reads from segment [${seg}] for [${db_name}/${col_name}/${bac_id}]"

      ln -s /usr/local/packages/clc-bfx-cell/license.properties ./

      set input_read_files = ""
      foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
        set input_read_files = `echo "${input_read_files} -q ${sample_seg_100x_sff_file:r}.${key}.sff"`
      end
      if ( -e ${sample_seg_sanger_file} ) then
        if ( `cat ${sample_seg_sanger_file} | wc -l` > 0 ) then
          set input_read_files = `echo "${input_read_files} -q ${sample_seg_sanger_file}"`
        endif
      endif
      clc_novo_assemble \
        -o ${seg}_100x_contigs.fasta \
        ${input_read_files} \
        >& ${seg}_100x_clc_novo_assemble.log

      set contig_cnt = `grep "^>" ${seg}_100x_contigs.fasta | wc -l`
      if ( ${contig_cnt} < 1 ) then
        echo "WARNING: clc de novo assembly of 100x coverage failed, trying cap3 for nonchimera reads from segment [${seg}] for [${db_name}/${col_name}/${bac_id}]"
        if ( -e ${db_name}_${col_name}_${bac_id}_nonchimera_${seg}_100x.fasta ) then
          rm ${db_name}_${col_name}_${bac_id}_nonchimera_${seg}_100x.fasta
        endif
        touch ${db_name}_${col_name}_${bac_id}_nonchimera_${seg}_100x.fasta
        foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
          sffinfo \
            -s ${sample_seg_100x_sff_file:r}.${key}.sff | \
            grep -v " length=0 " \
            >> ${db_name}_${col_name}_${bac_id}_nonchimera_${seg}_100x.fasta
        end
        if ( -e ${sample_seg_sanger_file} ) then
          cat ${sample_seg_sanger_file} >> ${db_name}_${col_name}_${bac_id}_nonchimera_${seg}_100x.fasta
        endif

        cap3 ${db_name}_${col_name}_${bac_id}_nonchimera_${seg}_100x.fasta
        mv ${db_name}_${col_name}_${bac_id}_nonchimera_${seg}_100x.fasta.cap.contigs ${seg}_100x_contigs.fasta
        rm ${db_name}_${col_name}_${bac_id}_nonchimera_${seg}_100x.fasta*
      endif
      
      set blast_db = ${blast_db_dir}/${seg}_full_length_NT_complete.fa
      set best_reference = ${seg_assembly_dir}/${seg}_best_reference.fna
      if ( -e ${seg}_100x_contigs.fasta ) then
        echo "INFO: finding best FL reference for segment [${seg}] for [${db_name}/${col_name}/${bac_id}]"
        set best_hit = \
          `blastall \
             -p blastn \
             -d ${blast_db} \
             -b 1 \
             -v 1 \
             -m 8 \
             -i ${seg}_100x_contigs.fasta | \
          sort -nrk12 | \
          head -n 1 | \
          gawk '{printf("%s\n", $2);}'`
        fastacmd -d ${blast_db} -p F -s "${best_hit}"  -o ${best_reference}
        grep "^>" ${best_reference} | cut -c 2- | gawk -v s=${seg} '{printf(">%s %s\n", s, $0);}' > ${best_reference}_mod
        grep -v "^>" ${best_reference} >> ${best_reference}_mod
        mv ${best_reference}_mod ${best_reference}
      else
        echo "ERROR: missing de novo assembly of 100x coverage for nonchimera reads from segment [${seg}] for [${db_name}/${col_name}/${bac_id}]"
      endif
    popd >& /dev/null
  end

  echo "INFO: consolidating best FL reference sequences for [${db_name}/${col_name}/${bac_id}]"
  set seg_best_ref_dir = ${sample_data}/reference_fasta
  if ( -d ${seg_best_ref_dir} ) then
  else
    mkdir -p ${seg_best_ref_dir}
  endif
  set best_refs_file = ${seg_best_ref_dir}/reference.fasta
  cat ${sample_data}/assembly_by_segment/*/*_best_reference.fna > ${best_refs_file}

  if ( ${flu_a} > 0 ) then
    echo "INFO: trimming mRT-PCR primer sites from best FL reference flu sequences for [${db_name}/${col_name}/${bac_id}]"
    pushd ${seg_best_ref_dir} >& /dev/null

      vectorstrip -sequence reference.fasta \
        -novectorfile \
        -mismatch 10 \
        -nobesthits \
        -linkera AGCRAAAGCAGG \
        -linkerb CCTTGTTTCTACT \
        -outfile reference.fasta.vectorstrip \
        -outseq reference_primer_trimmed.fasta

      grep "No match" reference.fasta.vectorstrip | \
        cut -f 1 | \
        cut -d ':' -f 2 | \
        tr -d ' ' > reference.fasta.includes.list

      fnafile -i reference.fasta.includes.list \
        -o reference_trimmed_final_unsorted.fasta \
        reference.fasta

      cat reference_primer_trimmed.fasta | \
        sed -e 's/_from_/ from_/g' >> reference_trimmed_final_unsorted.fasta

      foreach seg ( `cat reference_trimmed_final_unsorted.fasta | grep "^>" | cut -d ' ' -f 1 | cut -c 2- | sort -u` )
        echo ${seg} > foo_include.txt
        fnafile -i foo_include.txt -o foo_include.fasta reference_trimmed_final_unsorted.fasta 
        cat foo_include.fasta >> reference_trimmed_final_sorted.fasta
      end

      mv reference.fasta reference.fasta_untrimmed_original
      mv reference_trimmed_final_sorted.fasta reference.fasta
    popd >& /dev/null
  endif

  echo "INFO: consolidating nonchimera 454 reads for [${db_name}/${col_name}/${bac_id}]"
  set non_chimera_list = ${tblastx_outdir}/nonchimera_reads.uaccno_list
  cat ${tblastx_outdir}/*_nonchimera_reads.uaccno_list > ${non_chimera_list}

  set deconvolved_sff = ${sample_data_merged_sff}/${db_name}_${col_name}_${bac_id}_nonchimera.sff
  foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
    sfffile -i ${non_chimera_list} \
      -o ${deconvolved_sff:r}.${key}.sff \
      ${sample_data_merged_sff_file:r}.${key}.sff
  end

  echo "INFO: mapping viral sequences for [${db_name}/${col_name}/${bac_id}]"
  set sample_mapping_dir = ${sample_data}/mapping
  if ( -d ${sample_mapping_dir} ) then
  else
    mkdir -p ${sample_mapping_dir}
  endif

  pushd ${sample_mapping_dir} >& /dev/null
    ln -s /usr/local/packages/clc-bfx-cell/license.properties ./

    echo "INFO: using clc_ref_assemble_long to find sff SNPs for [${db_name}_${col_name}_${bac_id}]"

    set input_read_files = ""
    foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
      set input_read_files = `echo "${input_read_files} -q ${deconvolved_sff:r}.${key}.sff"`
    end
    clc_ref_assemble_long \
      -s 0.95 \
      -o ${db_name}_${col_name}_${bac_id}_454_only_gb_refs.cas \
      ${input_read_files} \
      -d ${best_refs_file}
    find_variations \
      -a ${db_name}_${col_name}_${bac_id}_454_only_gb_refs.cas \
      -c 2 \
      -o ${db_name}_${col_name}_${bac_id}_454_only_gb_refs.new_contigs \
      -v \
      -f 0.2 >& ${db_name}_${col_name}_${bac_id}_454_only_gb_refs_find_variations.log
    cat ${db_name}_${col_name}_${bac_id}_454_only_gb_refs_find_variations.log | \
      grep -v Nochange | \
      cut -d ':' -f 1 | \
      gawk '{if($0 ~ /^[A-Z]/){s=$1;n=0; } \
             else if ($0 ~ /Difference/){l=$1; c=$5; n=0; printf("%s:%d:%s\n", s, l, c);}}' > \
      ${db_name}_${col_name}_${bac_id}_454_only_gb_refs_find_variations.log.reduced

    touch ${db_name}_${col_name}_${bac_id}_solexa_only_gb_refs_find_variations.log.reduced
    if ( `cat ${sample_data_merged_solexa_file} | wc -l` > 0 ) then
      echo "INFO: using clc_ref_assemble_long to find fastq SNPs for [${db_name}_${col_name}_${bac_id}]"
      clc_ref_assemble_long \
        -s 0.95 \
        -o ${db_name}_${col_name}_${bac_id}_solexa_only_gb_refs.cas \
        -q ${sample_data_merged_solexa_file} \
        -d ${best_refs_file}
      find_variations \
        -a ${db_name}_${col_name}_${bac_id}_solexa_only_gb_refs.cas \
        -c 2 \
        -o ${db_name}_${col_name}_${bac_id}_solexa_only_gb_refs.new_contigs \
        -v \
        -f 0.2 >& ${db_name}_${col_name}_${bac_id}_solexa_only_gb_refs_find_variations.log
      cat ${db_name}_${col_name}_${bac_id}_solexa_only_gb_refs_find_variations.log | \
        grep -v Nochange | \
        cut -d ':' -f 1 | \
        gawk '{if($0 ~ /^[A-Z]/){s=$1;n=0; } \
               else if ($0 ~ /Difference/){l=$1; c=$5; n=0; printf("%s:%d:%s\n", s, l, c);}}' > \
        ${db_name}_${col_name}_${bac_id}_solexa_only_gb_refs_find_variations.log.reduced
    endif

    if ( `cat ${db_name}_${col_name}_${bac_id}_solexa_only_gb_refs_find_variations.log.reduced | wc -l` > 0 ) then
      comm -12 \
        ${db_name}_${col_name}_${bac_id}_454_only_gb_refs_find_variations.log.reduced \
        ${db_name}_${col_name}_${bac_id}_solexa_only_gb_refs_find_variations.log.reduced > \
        ${db_name}_${col_name}_${bac_id}_454_solexa_common_gb_refs_find_variations.log.reduced
    else
      cp \
        ${db_name}_${col_name}_${bac_id}_454_only_gb_refs_find_variations.log.reduced \
        ${db_name}_${col_name}_${bac_id}_454_solexa_common_gb_refs_find_variations.log.reduced
    endif 

    echo "INFO: building edited references based on common sff and fastq SNPs for [${db_name}_${col_name}_${bac_id}]"
    foreach seg ( `grep "^>" ${best_refs_file} | cut -d ' ' -f 1 | cut -c 2-` )
      nthseq -sequence ${best_refs_file} \
        -number `grep "^>" ${best_refs_file} | cut -d ' ' -f 1 | cut -c 2- | grep -n ${seg} | cut -d ':' -f 1` \
        -outseq ${db_name}_${col_name}_${bac_id}_${seg}.extracted >& /dev/null
      cat ${db_name}_${col_name}_${bac_id}_454_solexa_common_gb_refs_find_variations.log.reduced | \
        grep ${seg} | \
        cut -d ':' -f 2-3 | \
        tr '\n ' ' ' > ${db_name}_${col_name}_${bac_id}_${seg}.edits
      /usr/local/devel/DAS/software/resequencing/prod/data_analysis/delta2seq.pl \
        -r ${db_name}_${col_name}_${bac_id}_${seg}.extracted \
        -f ${db_name}_${col_name}_${bac_id}_${seg}.edits \
        -q ${db_name}_${col_name}_${bac_id}_${seg}.extracted.edited
      grep "^>" ${db_name}_${col_name}_${bac_id}_${seg}.extracted > \
        ${db_name}_${col_name}_${bac_id}_${seg}.extracted.edited.fasta
      grep -v "^>" ${db_name}_${col_name}_${bac_id}_${seg}.extracted.edited >> \
        ${db_name}_${col_name}_${bac_id}_${seg}.extracted.edited.fasta
    end
    set best_edited_refs_file = ${db_name}_${col_name}_${bac_id}_reference_edited.fasta
    cat ${db_name}_${col_name}_${bac_id}_*.extracted.edited.fasta > \
      ${best_edited_refs_file}

    echo "INFO: using 454 mapper for final chimera check for [${db_name}_${col_name}_${bac_id}]"

    if ( -d 454_mapping_best_refs_chimera_check ) then
      rm -Rf 454_mapping_best_refs_chimera_check
    endif
    newMapping 454_mapping_best_refs_chimera_check
    setRef 454_mapping_best_refs_chimera_check ${best_edited_refs_file}

    foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
      addRun 454_mapping_best_refs_chimera_check ${deconvolved_sff:r}.${key}.sff
    end

    if ( `cat ${sample_data_merged_solexa_file}.fasta | wc -l` > 0 ) then
      addRun 454_mapping_best_refs_chimera_check ${sample_data_merged_solexa_file}.fasta
    endif
    runProject -no 454_mapping_best_refs_chimera_check >& runProject_454_mapping_best_refs_chimera_check.log
    grep "Chimeric" 454_mapping_best_refs_chimera_check/mapping/454ReadStatus.txt | \
      gawk '{print $1}' > exclude_list.txt

    cat ${sample_data_merged_solexa_file_t} | gawk -F'\t' '{if(($2!=29)||($3!=100)){print $1;}}' >> exclude_list.txt

    set final_sff_reads = ${db_name}_${col_name}_${bac_id}_final.sff
    set final_fastq_reads = ${db_name}_${col_name}_${bac_id}_final.fastq
    set final_fasta_reads = ${db_name}_${col_name}_${bac_id}_final.fasta

    if ( `cat ${sample_data_merged_sanger_file} | wc -l` > 0 ) then
      cp ${sample_data_merged_sanger_file} ${final_fasta_reads}
      cp ${sample_data_merged_sanger_file}.untrimmed ${final_fasta_reads}.untrimmed
      cp ${sample_data_merged_sanger_file}.trimpoints ${final_fasta_reads}.trimpoints
    endif

    foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
      sfffile \
        -o ${final_sff_reads:r}.${key}.sff \
        -e exclude_list.txt \
        ${deconvolved_sff:r}.${key}.sff
    end

    touch ${final_fastq_reads}
    touch ${final_fastq_reads}.trimpoints
    touch ${final_fastq_reads}.untrimmed

    if ( `cat ${sample_data_merged_solexa_file} | wc -l` > 0 ) then
      /usr/local/devel/DAS/software/JavaCommon2/fastQfile.pl \
        -o ${final_fastq_reads} \
        -e exclude_list.txt \
        ${sample_data_merged_solexa_file}

      cat ${final_fastq_reads} | \
        gawk '{t=NR % 4;\
               if(t==1){\
                 if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,qid,q)};\
                 sid=substr($0,2);\
               }\
               else if (t==2){s=$0;}\
               else if (t==3){qid=substr($0,2);}\
               else if (t==0){q=$0;}\
              }\
              END {\
                if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,qid,q)};\
                sid=substr($0,2);\
              }' | \
        sort | \
        gawk -F'\t' '{printf("@%s\n%s\n+%s\n%s\n", $1, $2, $3, $4);}' > ${final_fastq_reads}.sorted
      mv ${final_fastq_reads} ${final_fastq_reads}.unsorted
      mv ${final_fastq_reads}.sorted ${final_fastq_reads}

      grep "^@" ${final_fastq_reads} | cut -c 2- | sort > include_list.txt
      join -1 1 -2 1 \
        include_list.txt \
        ${sample_data_merged_solexa_file_t} | \
        tr ' ' '\t' > ${final_fastq_reads}.trimpoints

      /usr/local/devel/DAS/software/JavaCommon2/fastQfile.pl \
        -o ${final_fastq_reads}.untrimmed \
        -i include_list.txt \
        ${sample_data_merged_solexa_file_u}

      cat ${final_fastq_reads}.untrimmed | \
        gawk '{t=NR % 4;\
               if(t==1){\
                 if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,qid,q)};\
                 sid=substr($0,2);\
               }\
               else if (t==2){s=$0;}\
               else if (t==3){qid=substr($0,2);}\
               else if (t==0){q=$0;}\
              }\
              END {\
                if(length(sid) > 0 ) {printf("%s\t%s\t%s\t%s\n", sid,s,qid,q)};\
                sid=substr($0,2);\
              }' | \
        sort | \
        gawk -F'\t' '{printf("@%s\n%s\n+%s\n%s\n", $1, $2, $3, $4);}' > ${final_fastq_reads}.untrimmed.sorted
      mv ${final_fastq_reads}.untrimmed.sorted ${final_fastq_reads}.untrimmed
    endif

    echo "INFO: running clc_ref_assemble_long for [${db_name}_${col_name}_${bac_id}]"

    set input_read_files = ""
    foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
      set input_read_files = `echo "${input_read_files} -q ${final_sff_reads:r}.${key}.sff"`
    end

    if ( `cat ${final_fasta_reads} | wc -l` > 0 ) then
      set input_read_files = `echo "${input_read_files} -q ${final_fasta_reads}"`
    endif

    if ( `cat ${final_fastq_reads} | wc -l` > 0 ) then
      set input_read_files = `echo "${input_read_files} -q ${final_fastq_reads}"`
    endif

    clc_ref_assemble_long \
      -s 0.95 \
      -o ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.cas \
      ${input_read_files} \
      -d ${best_edited_refs_file}

    find_variations \
      -a ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.cas \
      -c 2 \
      -o ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.new_contigs \
      -v \
      -f 0.2 >& ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs_find_variations.log

    if ( ${flu_a} > 0 ) then
      echo "INFO: running fluValidator for [${db_name}_${col_name}_${bac_id}]"
      /usr/local/devel/DAS/software/ElviraStaging/bin/fluValidator \
        --fasta ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.new_contigs > \
        ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.new_contigs.fluValidator 
    endif
  popd >& /dev/null
end

################### INITIALIZE AND LOAD SAMPLE TRACKING DATA #############################
set sispa_pool_name = 20090205_HIsamples6to100
set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sispa_pool_name = 20091005_AVIAN113
set sispa_pool_name = 20091215_MCEIRSsamples1to50
set sispa_pool_name = 20100305_1_86xAK_1xSW
set sispa_pool_name = 20100305_2_32xAK_24xCOH_1xMCWS_2xKHBAT_1xSW
set sispa_pool_name = 20100416_B_57xMCE_14xAK_5xCOH_4xVEEV_3xINS_1xCC_1xWKS

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt
foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':'` )
  set bc       = `echo "${bc_rec}" | cut -d ':' -f 1`
  set bc_seq   = `echo "${bc_rec}" | cut -d ':' -f 2`
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set blinded  = `echo "${bc_rec}" | cut -d ':' -f 4`
  set species  = `echo "${bc_rec}" | cut -d ':' -f 5`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 6`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 7`
  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}
  set sample_mapping_dir = ${sample_data}/mapping
  set sample_tracking_jar = /usr/local/projects/CAMERA/SampleTracking/SampleTracking.jar
  set java_cmd = /usr/local/java/1.6.0/bin/java
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -i

  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -U sampleDatabase="${db_name}"
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -U sampleCollection="${col_name}"
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -U sampleBacId="${bac_id}"
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -U sampleBlindedNumber="${blinded}"

  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -w 1
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -w 2
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -w 3
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -w 4
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -w 5
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -w 6
  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -w 7

end


  ${java_cmd} -jar ${sample_tracking_jar} -s "${db_name}_${col_name}_${bac_id}" -c "454 data processed and assembled, waiting on Solexa data"

################### REVIEW FLU VALIDATOR RESULTS FOR CLC DATA #############################
set sispa_pool_name = 20090205_HIsamples6to100
set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sispa_pool_name = 20091005_AVIAN113
set sispa_pool_name = 20091215_MCEIRSsamples1to50
set sispa_pool_name = 20100305_1_86xAK_1xSW
set sispa_pool_name = 20100305_2_32xAK_24xCOH_1xMCWS_2xKHBAT_1xSW
set sispa_pool_name = 20100416_B_57xMCE_14xAK_5xCOH_4xVEEV_3xINS_1xCC_1xWKS

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt
foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':'` )
  set bc       = `echo "${bc_rec}" | cut -d ':' -f 1`
  set bc_seq   = `echo "${bc_rec}" | cut -d ':' -f 2`
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set blinded  = `echo "${bc_rec}" | cut -d ':' -f 4`
  set species  = `echo "${bc_rec}" | cut -d ':' -f 5`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 6`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 7`
  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}
  set sample_mapping_dir = ${sample_data}/mapping
  pushd ${sample_mapping_dir} >& /dev/null
    if ( -e ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.new_contigs.fluValidator ) then
      cat ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.new_contigs.fluValidator 
    else if ( -e ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.new_contigs ) then
      echo "INFO: running fluValidator for [${db_name}_${col_name}_${bac_id}]"
      /usr/local/devel/DAS/software/ElviraStaging/bin/fluValidator \
        --fasta ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.new_contigs > \
        ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.new_contigs.fluValidator 
      cat ${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.new_contigs.fluValidator 
    endif
  popd >& /dev/null
end

################### VALIDATE CAS2CONSED DATA #############################
set sispa_pool_name = 20090205_HIsamples6to100
set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sispa_pool_name = 20091005_AVIAN113
set sispa_pool_name = 20091215_MCEIRSsamples1to50
set sispa_pool_name = 20100305_1_86xAK_1xSW
set sispa_pool_name = 20100305_2_32xAK_24xCOH_1xMCWS_2xKHBAT_1xSW
set sispa_pool_name = 20100416_B_57xMCE_14xAK_5xCOH_4xVEEV_3xINS_1xCC_1xWKS

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt

foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':' | cut -d ':' -f 3,6,7 | sort -u` )
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 1`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 2`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 3`
  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}
  set sample_mapping_dir = ${sample_data}/mapping

  if ( -e ${sample_mapping_dir}/consed_with_sanger/edit_dir/cas2consed.ace.1 ) then
    pushd ${sample_mapping_dir} >& /dev/null
      set ace_true_contig_count = `head -n 1 consed_with_sanger/edit_dir/cas2consed.ace.1 | gawk '{print $2}'`
      set ace_true_read_count = `head -n 1 consed_with_sanger/edit_dir/cas2consed.ace.1 | gawk '{print $3}'`
      set ace_contig_count = `grep "^CO" consed_with_sanger/edit_dir/cas2consed.ace.1 | wc -l`
      set ace_mapped_rd_count = `grep "^RD" consed_with_sanger/edit_dir/cas2consed.ace.1 | wc -l`
      set ace_mapped_af_count = `grep "^AF" consed_with_sanger/edit_dir/cas2consed.ace.1 | wc -l`

      if ( ${ace_contig_count} == ${ace_true_contig_count} ) then
      else
        echo "ERROR: ACE contig count [${ace_contig_count}] is less than ACE TRUE contig count [${ace_true_contig_count}] for sample [${db_name}_${col_name}_${bac_id}]"
      endif

      if ( ${ace_mapped_rd_count} == ${ace_true_read_count} ) then
      else
        echo "ERROR: ACE RD count [${ace_mapped_rd_count}] is not equal to ACE TRUE read count [${ace_true_read_count}] for sample [${db_name}_${col_name}_${bac_id}]"
      endif

      if ( ${ace_mapped_af_count} == ${ace_true_read_count} ) then
      else
        echo "ERROR: ACE AF count [${ace_mapped_af_count}] is not equal to ACE TRUE read count [${ace_true_read_count}] for sample [${db_name}_${col_name}_${bac_id}]"
      endif

      if ( ( ${ace_contig_count} == ${ace_true_contig_count} ) && \
           ( ${ace_mapped_rd_count} == ${ace_true_read_count} ) && \
           ( ${ace_mapped_af_count} == ${ace_true_read_count} ) ) then
        set clc_contig_count = `assembly_table *hybrid*.cas | grep -v "\-1" | gawk '{print $5}' | sort -n | uniq -c | wc -l`
        set clc_mapped_read_count = `assembly_table *hybrid*.cas | grep -v "\-1" | wc -l`
        if ( ${ace_contig_count} < ${clc_contig_count} ) then
          echo "WARNING: ACE contig count [${ace_contig_count}] is less than CAS contig count [${clc_contig_count}] for sample [${db_name}_${col_name}_${bac_id}]"
        endif

        if ( ${ace_mapped_rd_count} < ${clc_mapped_read_count} ) then
          echo "WARNING: ACE RD count [${ace_mapped_rd_count}] is less than CAS mapped read count [${clc_mapped_read_count}] for sample [${db_name}_${col_name}_${bac_id}]"
        endif

        if ( ${ace_mapped_af_count} < ${clc_mapped_read_count} ) then
          echo "WARNING: ACE AF count [${ace_mapped_af_count}] is less than CAS mapped read count [${clc_mapped_read_count}] for sample [${db_name}_${col_name}_${bac_id}]"
        endif
      else
        ls -l ${sample_mapping_dir}/consed_with_sanger/edit_dir/cas2consed.ace.1
      endif
    popd >& /dev/null
  else
    echo "WARNING: No ACE file exists for sample [${db_name}_${col_name}_${bac_id}]"
  endif
end

foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':' | cut -d ':' -f 3,6,7 | sort -u` )
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 1`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 2`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 3`
  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}
  set sample_mapping_dir = ${sample_data}/mapping
  echo "INFO:  Reviewing sample [${db_name}/${col_name}/${bac_id}]"
  
  if ( -e ${sample_mapping_dir}/consed_with_sanger/edit_dir/cas2consed.ace.1 ) then
    pushd ${sample_mapping_dir} >& /dev/null
      set ace_true_contig_count = `head -n 1 consed_with_sanger/edit_dir/cas2consed.ace.1 | gawk '{print $2}'`
      set ace_true_read_count = `head -n 1 consed_with_sanger/edit_dir/cas2consed.ace.1 | gawk '{print $3}'`
      set ace_contig_count = `grep "^CO" consed_with_sanger/edit_dir/cas2consed.ace.1 | wc -l`
      set ace_mapped_rd_count = `grep "^RD" consed_with_sanger/edit_dir/cas2consed.ace.1 | wc -l`
      set ace_mapped_af_count = `grep "^AF" consed_with_sanger/edit_dir/cas2consed.ace.1 | wc -l`

      if ( ${ace_contig_count} == ${ace_true_contig_count} ) then
      else
        echo "ERROR: ACE contig count [${ace_contig_count}] is less than ACE TRUE contig count [${ace_true_contig_count}] for sample [${db_name}_${col_name}_${bac_id}]"
      endif

      if ( ${ace_mapped_rd_count} == ${ace_true_read_count} ) then
      else
        echo "ERROR: ACE RD count [${ace_mapped_rd_count}] is not equal to ACE TRUE read count [${ace_true_read_count}] for sample [${db_name}_${col_name}_${bac_id}]"
      endif

      if ( ${ace_mapped_af_count} == ${ace_true_read_count} ) then
      else
        echo "ERROR: ACE AF count [${ace_mapped_af_count}] is not equal to ACE TRUE read count [${ace_true_read_count}] for sample [${db_name}_${col_name}_${bac_id}]"
      endif
    popd >& /dev/null
  else
    if ( -d ${sample_mapping_dir} ) then
      pushd ${sample_mapping_dir} >& /dev/null
        set clc_contig_count = `assembly_table *hybrid*.cas | grep -v "\-1" | gawk '{print $5}' | sort -n | uniq -c | wc -l`
        set clc_mapped_read_count = `assembly_table *hybrid*.cas | grep -v "\-1" | wc -l`
        if ( ${clc_contig_count} > 0 && ${clc_mapped_read_count} > 0 ) then
          echo "ERROR:  CAS has [${clc_contig_count}] contigs and [${clc_mapped_read_count}] mapped reads, but no ACE file exists for sample [${db_name}_${col_name}_${bac_id}]"
        endif
      popd >& /dev/null
    else
      echo "WARNING:  Directory [${sample_mapping_dir}] does not exist"
    endif
  endif
end

################### GENERATE FLU VALIDATOR RESULTS FOR CAS2CONSED DATA #############################
set sispa_pool_name = 20090205_HIsamples6to100
set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sispa_pool_name = 20091005_AVIAN113
set sispa_pool_name = 20091215_MCEIRSsamples1to50
set sispa_pool_name = 20100305_1_86xAK_1xSW
set sispa_pool_name = 20100305_2_32xAK_24xCOH_1xMCWS_2xKHBAT_1xSW
set sispa_pool_name = 20100416_B_57xMCE_14xAK_5xCOH_4xVEEV_3xINS_1xCC_1xWKS

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt

foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':' | cut -d ':' -f 3,6,7 | sort -u` )
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 1`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 2`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 3`

foreach bc_rec ( `cat /home/tstockwe/for_avian_flu/20100730_data_deliveries/giv3_samples_with_edits.txt | tr ',' ':' | sort -u` )
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 1`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 2`

  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}
  set sample_mapping_dir = ${sample_data}/mapping
  if ( -e ${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta ) then
    if ( -z ${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta ) then
      echo "ERROR:  [${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta] is empty file"
    else

      if ( -e ${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta.fluValidator ) then
        if ( -z ${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta.fluValidator ) then
          pushd ${sample_mapping_dir}/consed_with_sanger >& /dev/null
            echo "INFO: running fluValidator on cas2consed.consensus.fasta for [${db_name}_${col_name}_${bac_id}]"
              /usr/local/devel/DAS/software/ElviraStaging/bin/fluValidator \
                --fasta cas2consed.consensus.fasta > \
                cas2consed.consensus.fasta.fluValidator 
              cat cas2consed.consensus.fasta.fluValidator 
            sleep 1
          popd >& /dev/null
        else
          echo "INFO:  [${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta.fluValidator] already exists"
        endif
      else
        pushd ${sample_mapping_dir}/consed_with_sanger >& /dev/null
          echo "INFO: running fluValidator on cas2consed.consensus.fasta for [${db_name}_${col_name}_${bac_id}]"
          /usr/local/devel/DAS/software/ElviraStaging/bin/fluValidator \
            --fasta cas2consed.consensus.fasta > \
            cas2consed.consensus.fasta.fluValidator 
            cat cas2consed.consensus.fasta.fluValidator 
          sleep 1
        popd >& /dev/null
      endif
    endif
  else
    echo "ERROR:  [${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta] does not exist"
  endif
end

################### REVIEW FLU VALIDATOR RESULTS FOR CAS2CONSED DATA #############################
set sispa_pool_name = 20090205_HIsamples6to100
set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sispa_pool_name = 20091005_AVIAN113
set sispa_pool_name = 20091215_MCEIRSsamples1to50
set sispa_pool_name = 20100305_1_86xAK_1xSW
set sispa_pool_name = 20100305_2_32xAK_24xCOH_1xMCWS_2xKHBAT_1xSW
set sispa_pool_name = 20100416_B_57xMCE_14xAK_5xCOH_4xVEEV_3xINS_1xCC_1xWKS

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt
if ( -e ${sispa_pool_name}_status.csv ) then
  rm ${sispa_pool_name}_status.csv
endif
touch ${sispa_pool_name}_status.csv
if ( -e ${sispa_pool_name}_status.log ) then
  rm ${sispa_pool_name}_status.log 
endif
touch ${sispa_pool_name}_status.log 
foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':'` )
  set bc       = `echo "${bc_rec}" | cut -d ':' -f 1`
  set bc_seq   = `echo "${bc_rec}" | cut -d ':' -f 2`
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set blinded  = `echo "${bc_rec}" | cut -d ':' -f 4`
  set species  = `echo "${bc_rec}" | cut -d ':' -f 5`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 6`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 7`
  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}
  set sample_mapping_dir = ${sample_data}/mapping

  if ( -e ${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta.fluValidator ) then
      echo "\n\nINFO: cas2consed.consensus.fasta.fluValidator for sample [${db_name}_${col_name}_${bac_id}]"
      cat ${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta.fluValidator
      echo "\n\nINFO: cas2consed.consensus.fasta.fluValidator for sample [${db_name}_${col_name}_${bac_id}]" >> ${sispa_pool_name}_status.log
      cat ${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta.fluValidator >> ${sispa_pool_name}_status.log
      set valid_cnt = `cat ${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta.fluValidator | grep " VALID " | wc -l`
      set contig_cnt = `cat ${sample_mapping_dir}/consed_with_sanger/cas2consed.consensus.fasta.fluValidator | grep "Contig " | wc -l`
      if ( ${valid_cnt} == 8 && ${contig_cnt} == 8 ) then
        echo "VALID,${valid_cnt},${contig_cnt},${db_name},${col_name},${bac_id}"
        echo "VALID,${valid_cnt},${contig_cnt},${db_name},${col_name},${bac_id}" >> ${sispa_pool_name}_status.log
        echo "VALID,${valid_cnt},${contig_cnt},${db_name},${col_name},${bac_id}" >> ${sispa_pool_name}_status.csv
      else
        echo "DRAFT,${valid_cnt},${contig_cnt},${db_name},${col_name},${bac_id}"
        echo "DRAFT,${valid_cnt},${contig_cnt},${db_name},${col_name},${bac_id}" >> ${sispa_pool_name}_status.log
        echo "DRAFT,${valid_cnt},${contig_cnt},${db_name},${col_name},${bac_id}" >> ${sispa_pool_name}_status.csv
      endif
    else
      echo "WARNING: NO cas2consed.consensus.fasta.fluValidator FILE FOUND FOR SAMPLE [${db_name}_${col_name}_${bac_id}]"
      echo "WARNING: NO cas2consed.consensus.fasta.fluValidator FILE FOUND FOR SAMPLE [${db_name}_${col_name}_${bac_id}]" >> ${sispa_pool_name}_status.log
    endif
end



################### CREATE CAS2CONSED LIST FOR DANNY #############################
set project_root = /usr/local/projects/VHTNGS
set sample_data_root = ${project_root}/sample_data_new

foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':'` )
  set bc       = `echo "${bc_rec}" | cut -d ':' -f 1`
  set bc_seq   = `echo "${bc_rec}" | cut -d ':' -f 2`
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set blinded  = `echo "${bc_rec}" | cut -d ':' -f 4`
  set species  = `echo "${bc_rec}" | cut -d ':' -f 5`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 6`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 7`
  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}
  set sample_mapping_dir = ${sample_data}/mapping
  if ( -e ${sample_mapping_dir}/${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.cas ) then
    echo "${sample_mapping_dir}/${db_name}_${col_name}_${bac_id}_hybrid_edited_refs.cas"
    echo "${db_name},${col_name},${bac_id}"
  endif
end


################### QUICKLY BUILD SANGER_ONLY VARIATIONS FILE ###############
csh
setenv PATH /usr/local/packages/clc-ngs-cell:/usr/local/packages/clc-bfx-cell:${PATH}

set sispa_pool_name = 20090205_HIsamples6to100
set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sispa_pool_name = 20091005_AVIAN113
set sispa_pool_name = 20091215_MCEIRSsamples1to50
set sispa_pool_name = 20100416_B_57xMCE_14xAK_5xCOH_4xVEEV_3xINS_1xCC_1xWKS

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt

foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':' ` )
  set bc       = `echo "${bc_rec}" | cut -d ':' -f 1`
  set bc_seq   = `echo "${bc_rec}" | cut -d ':' -f 2`
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set blinded  = `echo "${bc_rec}" | cut -d ':' -f 4`
  set species  = `echo "${bc_rec}" | cut -d ':' -f 5`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 6`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 7`

  echo "INFO: processing data for [${db_name}/${col_name}/${bac_id}]"
  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}

  set sample_mapping_dir = ${sample_data}/mapping
  if ( -d ${sample_mapping_dir} ) then
  else
    mkdir -p ${sample_mapping_dir}
  endif

  pushd ${sample_mapping_dir} >& /dev/null
    ln -s /usr/local/packages/clc-bfx-cell/license.properties ./

    set best_edited_refs_file = ${db_name}_${col_name}_${bac_id}_reference_edited.fasta
    set final_fasta_reads = ${db_name}_${col_name}_${bac_id}_final.fasta

    if ( -e ${final_fasta_reads} ) then
      if ( `cat ${final_fasta_reads} | wc -l` > 0 ) then
        echo "INFO: using clc_ref_assemble_long to find SANGER SNPs for [${db_name}_${col_name}_${bac_id}]"
        clc_ref_assemble_long \
          -s 0.95 \
          -o ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs.cas \
          -q ${final_fasta_reads} \
          -d ${best_edited_refs_file}
        find_variations \
          -a ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs.cas \
          -c 2 \
          -o ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs.new_contigs \
          -v \
          -f 0.2 >& ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs_find_variations.log
        clc_ref_assemble_long \
          -s 0.95 \
          -o ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed.cas \
          -q ${final_fasta_reads} \
          -d consed_with_sanger/cas2consed.consensus.fasta
        find_variations \
          -a ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed.cas \
          -c 2 \
          -o ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed.new_contigs \
          -v \
          -f 0.2 >& ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed_find_variations.log
      endif
    endif
  popd >& /dev/null
end

################### QUICKLY BUILD 454_ONLY, SOLEXA_ONLY, and SANGER_ONLY VARIATIONS FILE ###############
csh
setenv PATH /usr/local/packages/clc-ngs-cell:/usr/local/packages/clc-bfx-cell:${PATH}

set sispa_pool_name = 20090205_HIsamples6to100
set sispa_pool_name = 20090205_34xDW_5xHI_2xUNKNOWN_samples
set sispa_pool_name = 20090901_20xDW09_3xCC_12xSW_samples
set sispa_pool_name = 20091005_AVIAN113
set sispa_pool_name = 20091215_MCEIRSsamples1to50

set project_root = /usr/local/projects/VHTNGS
set barcode_data_root = ${project_root}/barcode_data
set sample_data_root = ${project_root}/sample_data_new
set barcode_data_dir = ${barcode_data_root}/${sispa_pool_name}
set barcode_file_name = ${barcode_data_dir}/barcode_metadata_from_GLK.txt

foreach bc_rec ( `cat ${barcode_file_name} | tr ' ' '_' | tr '\t' ':' ` )
  set bc       = `echo "${bc_rec}" | cut -d ':' -f 1`
  set bc_seq   = `echo "${bc_rec}" | cut -d ':' -f 2`
  set bac_id   = `echo "${bc_rec}" | cut -d ':' -f 3`
  set blinded  = `echo "${bc_rec}" | cut -d ':' -f 4`
  set species  = `echo "${bc_rec}" | cut -d ':' -f 5`
  set db_name  = `echo "${bc_rec}" | cut -d ':' -f 6`
  set col_name = `echo "${bc_rec}" | cut -d ':' -f 7`

  echo "INFO: processing data for [${db_name}/${col_name}/${bac_id}]"
  set sample_data = ${sample_data_root}/${db_name}/${col_name}/${bac_id}
  set sample_data_merged_sff = ${sample_data}/merged_sff

  set sample_mapping_dir = ${sample_data}/mapping
  if ( -d ${sample_mapping_dir} ) then
  else
    mkdir -p ${sample_mapping_dir}
  endif

  pushd ${sample_mapping_dir} >& /dev/null
    ln -s /usr/local/packages/clc-bfx-cell/license.properties ./

    set best_edited_refs_file = ${db_name}_${col_name}_${bac_id}_reference_edited.fasta
    set final_sff_reads = ${db_name}_${col_name}_${bac_id}_final.sff
    set final_fastq_reads = ${db_name}_${col_name}_${bac_id}_final.fastq
    set final_fasta_reads = ${db_name}_${col_name}_${bac_id}_final.fasta

    set input_read_files = ""
    foreach key (`ls -1 ${sample_data_merged_sff} | grep "\.[ACGT][ACGT][ACGT][ACGT]\." | cut -d '.' -f 2 | sort -u`)
      set input_read_files = `echo "${input_read_files} -q ${final_sff_reads:r}.${key}.sff"`
    end

    if ( `echo ${input_read_files} | wc -c` > 0 ) then
      echo "INFO: using clc_ref_assemble_long to find 454 SNPs for [${db_name}_${col_name}_${bac_id}]"
      clc_ref_assemble_long \
        -s 0.95 \
        -o ${db_name}_${col_name}_${bac_id}_454_only_edited_refs.cas \
        ${input_read_files} \
        -d ${best_edited_refs_file}
      find_variations \
        -a ${db_name}_${col_name}_${bac_id}_454_only_edited_refs.cas \
        -c 2 \
        -o ${db_name}_${col_name}_${bac_id}_454_only_edited_refs.new_contigs \
        -v \
        -f 0.2 >& ${db_name}_${col_name}_${bac_id}_454_only_edited_refs_find_variations.log
      cat ${db_name}_${col_name}_${bac_id}_454_only_edited_refs_find_variations.log | \
        grep -v Nochange | \
        cut -d ':' -f 1 | \
        gawk '{if($0 ~ /^[A-Z]/){s=$1;n=0; } \
               else if ($0 ~ /Difference/){l=$1; c=$5; n=0; printf("%s:%d:%s\n", s, l, c);}}' > \
          ${db_name}_${col_name}_${bac_id}_454_only_edited_refs_find_variations.log.reduced
      clc_ref_assemble_long \
        -s 0.95 \
        -o ${db_name}_${col_name}_${bac_id}_454_only_cas2consed.cas \
        ${input_read_files} \
        -d consed_with_sanger/cas2consed.consensus.fasta
      find_variations \
        -a ${db_name}_${col_name}_${bac_id}_454_only_cas2consed.cas \
        -c 2 \
        -o ${db_name}_${col_name}_${bac_id}_454_only_cas2consed.new_contigs \
        -v \
        -f 0.2 >& ${db_name}_${col_name}_${bac_id}_454_only_cas2consed_find_variations.log
      cat ${db_name}_${col_name}_${bac_id}_454_only_cas2consed_find_variations.log | \
        grep -v Nochange | \
        cut -d ':' -f 1 | \
        gawk '{if($0 ~ /^[A-Z]/){s=$1;n=0; } \
               else if ($0 ~ /Difference/){l=$1; c=$5; n=0; printf("%s:%d:%s\n", s, l, c);}}' > \
          ${db_name}_${col_name}_${bac_id}_454_only_cas2consed_find_variations.log.reduced
    endif

    if ( -e ${final_fastq_reads} ) then
      echo "INFO: using clc_ref_assemble_long to find SOLEXA SNPs for [${db_name}_${col_name}_${bac_id}]"
      clc_ref_assemble_long \
        -s 0.95 \
        -o ${db_name}_${col_name}_${bac_id}_solexa_only_edited_refs.cas \
        -q ${final_fastq_reads} \
        -d ${best_edited_refs_file}
      find_variations \
        -a ${db_name}_${col_name}_${bac_id}_solexa_only_edited_refs.cas \
        -c 2 \
        -o ${db_name}_${col_name}_${bac_id}_solexa_only_edited_refs.new_contigs \
        -v \
        -f 0.2 >& ${db_name}_${col_name}_${bac_id}_solexa_only_edited_refs_find_variations.log
      cat ${db_name}_${col_name}_${bac_id}_solexa_only_edited_refs_find_variations.log | \
        grep -v Nochange | \
        cut -d ':' -f 1 | \
        gawk '{if($0 ~ /^[A-Z]/){s=$1;n=0; } \
               else if ($0 ~ /Difference/){l=$1; c=$5; n=0; printf("%s:%d:%s\n", s, l, c);}}' > \
          ${db_name}_${col_name}_${bac_id}_solexa_only_edited_refs_find_variations.log.reduced
      clc_ref_assemble_long \
        -s 0.95 \
        -o ${db_name}_${col_name}_${bac_id}_solexa_only_cas2consed.cas \
        -q ${final_fastq_reads} \
        -d consed_with_sanger/cas2consed.consensus.fasta
      find_variations \
        -a ${db_name}_${col_name}_${bac_id}_solexa_only_cas2consed.cas \
        -c 2 \
        -o ${db_name}_${col_name}_${bac_id}_solexa_only_cas2consed.new_contigs \
        -v \
        -f 0.2 >& ${db_name}_${col_name}_${bac_id}_solexa_only_cas2consed_find_variations.log
      cat ${db_name}_${col_name}_${bac_id}_solexa_only_cas2consed_find_variations.log | \
        grep -v Nochange | \
        cut -d ':' -f 1 | \
        gawk '{if($0 ~ /^[A-Z]/){s=$1;n=0; } \
               else if ($0 ~ /Difference/){l=$1; c=$5; n=0; printf("%s:%d:%s\n", s, l, c);}}' > \
          ${db_name}_${col_name}_${bac_id}_solexa_only_cas2consed_find_variations.log.reduced
    endif

    if ( -e ${final_fasta_reads} ) then
      if ( `cat ${final_fasta_reads} | wc -l` > 0 ) then
        echo "INFO: using clc_ref_assemble_long to find SANGER SNPs for [${db_name}_${col_name}_${bac_id}]"
        clc_ref_assemble_long \
          -s 0.95 \
          -o ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs.cas \
          -q ${final_fasta_reads} \
          -d ${best_edited_refs_file}
        find_variations \
          -a ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs.cas \
          -c 2 \
          -o ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs.new_contigs \
          -v \
          -f 0.2 >& ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs_find_variations.log
      cat ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs_find_variations.log | \
        gawk '{if($0 ~ /^[A-Z]/){s=$1;n=0; } \
               else if ($0 ~ /Difference/){l=$1; c=$5; n=0; printf("%s:%d:%s\n", s, l, c);}}' > \
          ${db_name}_${col_name}_${bac_id}_sanger_only_edited_refs_find_variations.log.reduced

        clc_ref_assemble_long \
          -s 0.95 \
          -o ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed.cas \
          -q ${final_fasta_reads} \
          -d consed_with_sanger/cas2consed.consensus.fasta
        find_variations \
          -a ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed.cas \
          -c 2 \
          -o ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed.new_contigs \
          -v \
          -f 0.2 >& ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed_find_variations.log
      cat ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed_find_variations.log | \
        gawk '{if($0 ~ /^[A-Z]/){s=$1;n=0; } \
               else if ($0 ~ /Difference/){l=$1; c=$5; n=0; printf("%s:%d:%s\n", s, l, c);}}' > \
          ${db_name}_${col_name}_${bac_id}_sanger_only_cas2consed_find_variations.log.reduced
      endif
    endif
  popd >& /dev/null
end









